{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a879055-2c93-4421-b3ef-d851f714036f",
      "metadata": {
        "id": "6a879055-2c93-4421-b3ef-d851f714036f"
      },
      "outputs": [],
      "source": [
        "# If you're in Colab, run this cell first\n",
        "!pip install -q \"transformers>=4.45.0\" \"datasets>=2.19.0\" \\\n",
        "  bitsandbytes peft trl qwen-vl-utils accelerate pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cad0b313-fe71-43d2-b4a2-bf023c3d4703",
      "metadata": {
        "id": "cad0b313-fe71-43d2-b4a2-bf023c3d4703",
        "outputId": "bee78d4a-6a8c-4c61-fecb-8440001fdf05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting hf_transfer\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf_transfer\n",
            "Successfully installed hf_transfer-0.1.9\n"
          ]
        }
      ],
      "source": [
        "!pip install hf_transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f48ccac-f5e7-485e-807c-866350380ef4",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "69e9cc600df24771bcf8544bc1215c9e",
            "6e8de1dbd10246869616eadbfcc21ce8",
            "6a916b8967104d57ae5b55c8394a37c4",
            "f2f6441cf5754352b3ef2ad578a9ba07",
            "731d72e69c44401fb05b2d4213871333",
            "f1054ba449a843a3b6ec5d538fb02528",
            "5bb70927a70e4928803bfc283c6157c4",
            "0bdcb1e9b9d24a51b55d4cba7f3a2cc6",
            "662cf8d1f8644eec87b476e60cfa2b67",
            "ccd78407f7de474bb40fed083162408c",
            "204e1da4245a41e78ed767c70a28cd8a",
            "6faea1d9d013488aa9c1b7990f747bbc",
            "f19557078cfc4a2abe5b7ea796a4eb35",
            "9e653660991e408f8176eb99cbcb5141",
            "971b5da9a91047f584038fc6c2b4d108",
            "213acb1dbac443dd8e3cdd82c939550b",
            "62b3ceb0de9c44d7ba3ce8c909fd96f7",
            "11e9fcb855bc480e944b12db35bfa5f2",
            "8fa4cf01f40b49068b871b4dd1304d2c",
            "ec75531800e84baea26934c26f7e1991",
            "2026c22789d440a3b10a139cf1f7e32c",
            "633d451725d34687a00793f93f03856b",
            "abf6eed91c894dcab537b38f1efcf84e"
          ]
        },
        "id": "0f48ccac-f5e7-485e-807c-866350380ef4",
        "outputId": "711fda2d-5354-4053-e80a-9a4b107deda9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69e9cc600df24771bcf8544bc1215c9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e8de1dbd10246869616eadbfcc21ce8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001-1028f23e353fbe(…):   0%|          | 0.00/377M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a916b8967104d57ae5b55c8394a37c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/validation-00000-of-00001-6c7328ff6(…):   0%|          | 0.00/126M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2f6441cf5754352b3ef2ad578a9ba07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/test-00000-of-00001-f0e719df791966f(…):   0%|          | 0.00/122M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "731d72e69c44401fb05b2d4213871333",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/12726 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1054ba449a843a3b6ec5d538fb02528",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/4241 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bb70927a70e4928803bfc283c6157c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/4241 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['image', 'question', 'choices', 'answer', 'hint', 'task', 'grade', 'subject', 'topic', 'category', 'skill', 'lecture', 'solution'],\n",
            "        num_rows: 12726\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['image', 'question', 'choices', 'answer', 'hint', 'task', 'grade', 'subject', 'topic', 'category', 'skill', 'lecture', 'solution'],\n",
            "        num_rows: 4241\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['image', 'question', 'choices', 'answer', 'hint', 'task', 'grade', 'subject', 'topic', 'category', 'skill', 'lecture', 'solution'],\n",
            "        num_rows: 4241\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bdcb1e9b9d24a51b55d4cba7f3a2cc6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/12726 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "662cf8d1f8644eec87b476e60cfa2b67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/4241 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccd78407f7de474bb40fed083162408c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/4241 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 6218\n",
            "Val size: 2097\n",
            "Test size: 2017\n",
            "Example keys: ['image', 'question', 'choices', 'answer', 'hint', 'task', 'grade', 'subject', 'topic', 'category', 'skill', 'lecture', 'solution']\n",
            "Formatting datasets...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example formatted messages:\n",
            "{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a helpful science tutor.\\nYou see a question, several answer choices, and often an image and a hint/lecture.\\nYour job is to pick the single correct multiple-choice option.\\nStart your answer with ONLY the letter of the correct option (A, B, C, D, etc.),\\nthen a period, then a short explanation.\\nExample: \"C. Because ...\"\\n'}]}\n",
            "{'role': 'user', 'content': [{'type': 'image', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=750x429 at 0x71BA5C85E150>}, {'type': 'text', 'text': 'Question: Which of these states is farthest north?\\n\\nChoices:\\n(A) West Virginia\\n(B) Louisiana\\n(C) Arizona\\n(D) Oklahoma\\nLecture: Maps have four cardinal directions, or main directions. Those directions are north, south, east, and west.\\nA compass rose is a set of arrows that point to the cardinal directions. A compass rose usually shows only the first letter of each cardinal direction.\\nThe north arrow points to the North Pole. On most maps, north is at the top of the map.\\n\\nRespond starting with the letter, then a period, then a brief explanation.'}]}\n",
            "{'role': 'assistant', 'content': [{'type': 'text', 'text': 'A. To find the answer, look at the compass rose. Look at which way the north arrow is pointing. West Virginia is farthest north.'}]}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "204e1da4245a41e78ed767c70a28cd8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6faea1d9d013488aa9c1b7990f747bbc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f19557078cfc4a2abe5b7ea796a4eb35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e653660991e408f8176eb99cbcb5141",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "971b5da9a91047f584038fc6c2b4d108",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "213acb1dbac443dd8e3cdd82c939550b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62b3ceb0de9c44d7ba3ce8c909fd96f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11e9fcb855bc480e944b12db35bfa5f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fa4cf01f40b49068b871b4dd1304d2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec75531800e84baea26934c26f7e1991",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2026c22789d440a3b10a139cf1f7e32c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "633d451725d34687a00793f93f03856b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1167' max='1167' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1167/1167 14:59, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Entropy</th>\n",
              "      <th>Num Tokens</th>\n",
              "      <th>Mean Token Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.096700</td>\n",
              "      <td>3.340326</td>\n",
              "      <td>3.467196</td>\n",
              "      <td>1737553.000000</td>\n",
              "      <td>0.541637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.062900</td>\n",
              "      <td>3.262531</td>\n",
              "      <td>3.380091</td>\n",
              "      <td>3462723.000000</td>\n",
              "      <td>0.557930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>3.206100</td>\n",
              "      <td>3.233695</td>\n",
              "      <td>3.345482</td>\n",
              "      <td>5181757.000000</td>\n",
              "      <td>0.564243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>3.412600</td>\n",
              "      <td>3.217886</td>\n",
              "      <td>3.329388</td>\n",
              "      <td>6934906.000000</td>\n",
              "      <td>0.567789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.371900</td>\n",
              "      <td>3.209956</td>\n",
              "      <td>3.318748</td>\n",
              "      <td>8673048.000000</td>\n",
              "      <td>0.569656</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Saved fine-tuned adapter to: qwen2-vl-2b-scienceqa-lora-expl\n",
            "Loading fine-tuned model for inference...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abf6eed91c894dcab537b38f1efcf84e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Quick validation example ===\n",
            "Question: Which animal's mouth is also adapted for bottom feeding?\n",
            "Choices: ['discus', 'armored catfish']\n",
            "True answer letter: B\n",
            "Model raw output:    B. Look at the picture of the sturgeon.\n",
            "The sturgeon has a long, thin mouth. Its mouth is adapted for bottom feeding. The sturgeon uses its mouth to catch small fish and other food that lives at the bottom of the water.\n",
            "Now look at each animal. Figure out which animal has a similar\n",
            "Parsed letter:       B\n",
            "Parsed explanation:  Look at the picture of the sturgeon.\n",
            "The sturgeon has a long, thin mouth. Its mouth is adapted for bottom feeding. The sturgeon uses its mouth to catch small fish and other food that lives at the bottom of the water.\n",
            "Now look at each animal. Figure out which animal has a similar\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# 0. (Optional) Install deps\n",
        "# ============================\n",
        "# If running in Colab or a fresh env, uncomment this block:\n",
        "# !pip install -q \"transformers>=4.45.0\" \"datasets>=2.19.0\" \\\n",
        "#   peft trl qwen-vl-utils accelerate pillow\n",
        "\n",
        "# ============================\n",
        "# 1. Imports & basic setup\n",
        "# ============================\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import (\n",
        "    Qwen2VLForConditionalGeneration,\n",
        "    Qwen2VLProcessor,\n",
        ")\n",
        "\n",
        "from peft import LoraConfig\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ============================\n",
        "# 2. Load ScienceQA dataset\n",
        "# ============================\n",
        "# We use derek-thomas/ScienceQA which includes images + QA fields\n",
        "dataset_id = \"derek-thomas/ScienceQA\"\n",
        "raw_datasets = load_dataset(dataset_id)\n",
        "\n",
        "print(raw_datasets)\n",
        "\n",
        "# Keep only examples that actually have an image\n",
        "def has_image(example):\n",
        "    return example[\"image\"] is not None\n",
        "\n",
        "train_ds = raw_datasets[\"train\"].filter(has_image)\n",
        "val_ds   = raw_datasets[\"validation\"].filter(has_image)\n",
        "test_ds  = raw_datasets[\"test\"].filter(has_image)\n",
        "\n",
        "print(\"Train size:\", len(train_ds))\n",
        "print(\"Val size:\", len(val_ds))\n",
        "print(\"Test size:\", len(test_ds))\n",
        "print(\"Example keys:\", train_ds.column_names)\n",
        "\n",
        "# ============================\n",
        "# 3. Format data: letter + explanation\n",
        "# ============================\n",
        "system_message = \"\"\"You are a helpful science tutor.\n",
        "You see a question, several answer choices, and often an image and a hint/lecture.\n",
        "Your job is to pick the single correct multiple-choice option.\n",
        "Start your answer with ONLY the letter of the correct option (A, B, C, D, etc.),\n",
        "then a period, then a short explanation.\n",
        "Example: \"C. Because ...\"\n",
        "\"\"\"\n",
        "\n",
        "LETTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "\n",
        "def index_to_letter(idx: int) -> str:\n",
        "    return LETTERS[idx]\n",
        "\n",
        "def build_question_text(example) -> str:\n",
        "    \"\"\"\n",
        "    Build the user-facing question text with choices, hint, lecture.\n",
        "    \"\"\"\n",
        "    choices = example[\"choices\"]\n",
        "    options_text = \"\\n\".join(\n",
        "        f\"({LETTERS[i]}) {choice}\" for i, choice in enumerate(choices)\n",
        "    )\n",
        "\n",
        "    hint = example.get(\"hint\", \"\")\n",
        "    lecture = example.get(\"lecture\", \"\")\n",
        "\n",
        "    hint_part = f\"\\nHint: {hint}\" if hint else \"\"\n",
        "    lecture_part = f\"\\nLecture: {lecture}\" if lecture else \"\"\n",
        "\n",
        "    return (\n",
        "        f\"Question: {example['question']}\\n\\n\"\n",
        "        f\"Choices:\\n{options_text}\"\n",
        "        f\"{hint_part}{lecture_part}\\n\\n\"\n",
        "        f\"Respond starting with the letter, then a period, then a brief explanation.\"\n",
        "    )\n",
        "\n",
        "def format_example(example):\n",
        "    \"\"\"\n",
        "    Convert a raw ScienceQA row into the format:\n",
        "      {\n",
        "        \"images\": [...],\n",
        "        \"messages\": [ {role, content: [{type, ...}, ...]}, ... ]\n",
        "      }\n",
        "    for Qwen2-VL + TRL SFTTrainer.\n",
        "    \"\"\"\n",
        "    correct_letter = index_to_letter(int(example[\"answer\"]))\n",
        "\n",
        "    # ScienceQA usually has a \"solution\" field (text explanation).\n",
        "    explanation = example.get(\"solution\", \"\") or example.get(\"explanation\", \"\")\n",
        "    if explanation:\n",
        "        assistant_text = f\"{correct_letter}. {explanation}\"\n",
        "    else:\n",
        "        # Fallback to letter only if no explanation\n",
        "        assistant_text = correct_letter\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": system_message},\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": example[\"image\"]},\n",
        "                {\"type\": \"text\", \"text\": build_question_text(example)},\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": assistant_text},\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"images\": [example[\"image\"]],\n",
        "        \"messages\": messages,\n",
        "    }\n",
        "\n",
        "print(\"Formatting datasets...\")\n",
        "train_data = [format_example(ex) for ex in train_ds]\n",
        "eval_data  = [format_example(ex) for ex in val_ds]\n",
        "test_data  = [format_example(ex) for ex in test_ds]\n",
        "\n",
        "print(\"Example formatted messages:\")\n",
        "print(train_data[0][\"messages\"][0])  # system\n",
        "print(train_data[0][\"messages\"][1])  # user\n",
        "print(train_data[0][\"messages\"][2])  # assistant\n",
        "\n",
        "# ============================\n",
        "# 4. Load Qwen2-VL-2B-Instruct (bf16, no quantization)\n",
        "# ============================\n",
        "model_id = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",          # H200 can handle this easily\n",
        "    torch_dtype=torch.bfloat16, # native bf16 on H200\n",
        ")\n",
        "\n",
        "processor = Qwen2VLProcessor.from_pretrained(model_id)\n",
        "\n",
        "# ============================\n",
        "# 5. LoRA configuration (no 4-bit needed)\n",
        "# ============================\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # standard for Qwen2-VL\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# 6. SFTTraining configuration\n",
        "# ============================\n",
        "output_dir = \"qwen2-vl-2b-scienceqa-lora-expl\"\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,      # H200: you can increase if you want\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,      # effective batch ~16\n",
        "    learning_rate=1e-4,                 # 1e-4–2e-4 typical for LoRA\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1000,\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=False,       # not needed on H200\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# 7. Build SFTTrainer and train\n",
        "# ============================\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=eval_data,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=processor,  # TRL picks a VLM collator for Qwen2-VL\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Save LoRA adapter (and tokenizer/processor config)\n",
        "trainer.save_model(output_dir)\n",
        "processor.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Saved fine-tuned adapter to: {output_dir}\")\n",
        "\n",
        "# ============================\n",
        "# 8. Inference: answer + explanation\n",
        "# ============================\n",
        "# Reload model + adapter (for clarity; you can also reuse `model` from above)\n",
        "print(\"Loading fine-tuned model for inference...\")\n",
        "ft_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "ft_model.load_adapter(output_dir)\n",
        "ft_model.eval()\n",
        "\n",
        "ft_processor = Qwen2VLProcessor.from_pretrained(model_id)\n",
        "\n",
        "def generate_answer_with_explanation(example, max_new_tokens=64):\n",
        "    \"\"\"\n",
        "    Run inference on a single ScienceQA example.\n",
        "    Returns text like:\n",
        "      \"C. Because ...\"\n",
        "    \"\"\"\n",
        "    # Format in the same way as training, but we only use system+user messages\n",
        "    formatted = format_example(example)\n",
        "    conv_for_gen = formatted[\"messages\"][:2]  # system + user\n",
        "\n",
        "    # Build chat-style prompt\n",
        "    text_prompt = ft_processor.apply_chat_template(\n",
        "        conv_for_gen,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    # Process vision inputs\n",
        "    image_inputs, _ = process_vision_info(formatted[\"messages\"])\n",
        "\n",
        "    inputs = ft_processor(\n",
        "        text=[text_prompt],\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gen_ids = ft_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "\n",
        "    # Remove prompt tokens from output\n",
        "    trimmed = [\n",
        "        out_ids[len(in_ids):]\n",
        "        for in_ids, out_ids in zip(inputs.input_ids, gen_ids)\n",
        "    ]\n",
        "\n",
        "    out_text = ft_processor.batch_decode(\n",
        "        trimmed,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False,\n",
        "    )[0].strip()\n",
        "\n",
        "    return out_text\n",
        "\n",
        "def parse_letter_and_explanation(text):\n",
        "    \"\"\"\n",
        "    Extract the leading letter (A/B/C/...) and the explanation.\n",
        "    Falls back gracefully if the pattern is weird.\n",
        "    \"\"\"\n",
        "    # Match patterns like:\n",
        "    #   \"C. Because ...\"\n",
        "    #   \"C - Because ...\"\n",
        "    #   \"C) Because ...\"\n",
        "    m = re.match(r\"\\s*([A-Z])\\s*[\\.\\-\\):]\\s*(.*)\", text, flags=re.DOTALL)\n",
        "    if m:\n",
        "        letter = m.group(1)\n",
        "        explanation = m.group(2).strip()\n",
        "    else:\n",
        "        # Fallback: first capital letter\n",
        "        m2 = re.search(r\"[A-Z]\", text)\n",
        "        if m2:\n",
        "            letter = m2.group(0)\n",
        "            explanation = text[m2.end():].strip()\n",
        "        else:\n",
        "            letter = None\n",
        "            explanation = text.strip()\n",
        "    return letter, explanation\n",
        "\n",
        "# ============================\n",
        "# 9. Quick test on validation example\n",
        "# ============================\n",
        "print(\"\\n=== Quick validation example ===\")\n",
        "example = val_ds[0]\n",
        "\n",
        "model_output = generate_answer_with_explanation(example)\n",
        "pred_letter, pred_expl = parse_letter_and_explanation(model_output)\n",
        "\n",
        "true_letter = index_to_letter(int(example[\"answer\"]))\n",
        "\n",
        "print(\"Question:\", example[\"question\"])\n",
        "print(\"Choices:\", example[\"choices\"])\n",
        "print(\"True answer letter:\", true_letter)\n",
        "print(\"Model raw output:   \", model_output)\n",
        "print(\"Parsed letter:      \", pred_letter)\n",
        "print(\"Parsed explanation: \", pred_expl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0171de4e-4800-4f71-99bc-7e362762bcf1",
      "metadata": {
        "id": "0171de4e-4800-4f71-99bc-7e362762bcf1",
        "outputId": "f7c43b19-38df-45a1-d0a5-31adbecdf5ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 200/200 [02:32<00:00,  1.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on 200 examples: 0.6650\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def eval_scienceqa_split(dataset, max_new_tokens=64, num_samples=None):\n",
        "    \"\"\"\n",
        "    Compute MC accuracy on a ScienceQA split (e.g. val_ds or test_ds).\n",
        "\n",
        "    Args:\n",
        "        dataset: Hugging Face dataset (val_ds/test_ds).\n",
        "        max_new_tokens: generation length (increase if explanations are truncated).\n",
        "        num_samples: limit for quick testing (e.g. 200). If None, use full split.\n",
        "    \"\"\"\n",
        "    n = len(dataset) if num_samples is None else min(num_samples, len(dataset))\n",
        "    correct = 0\n",
        "\n",
        "    for i in tqdm(range(n), desc=\"Evaluating\"):\n",
        "        ex = dataset[i]\n",
        "        true_letter = index_to_letter(int(ex[\"answer\"]))\n",
        "\n",
        "        out_text = generate_answer_with_explanation(ex, max_new_tokens=max_new_tokens)\n",
        "        pred_letter, _ = parse_letter_and_explanation(out_text)\n",
        "\n",
        "        if pred_letter == true_letter:\n",
        "            correct += 1\n",
        "\n",
        "    acc = correct / n\n",
        "    print(f\"Accuracy on {n} examples: {acc:.4f}\")\n",
        "    return acc\n",
        "\n",
        "# Quick check on first 200 validation examples\n",
        "val_acc_200 = eval_scienceqa_split(val_ds, max_new_tokens=64, num_samples=200)\n",
        "\n",
        "# Full validation accuracy (comment in when you're ready)\n",
        "# val_acc_full = eval_scienceqa_split(val_ds, max_new_tokens=64, num_samples=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da1b2802-118e-4ca6-b75a-ddc22504a5f7",
      "metadata": {
        "id": "da1b2802-118e-4ca6-b75a-ddc22504a5f7"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def build_custom_question_text(question, choices, hint=None, lecture=None):\n",
        "    options_text = \"\\n\".join(\n",
        "        f\"({LETTERS[i]}) {choice}\" for i, choice in enumerate(choices)\n",
        "    )\n",
        "    hint_part = f\"\\nHint: {hint}\" if hint else \"\"\n",
        "    lecture_part = f\"\\nLecture: {lecture}\" if lecture else \"\"\n",
        "\n",
        "    return (\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        f\"Choices:\\n{options_text}\"\n",
        "        f\"{hint_part}{lecture_part}\\n\\n\"\n",
        "        f\"Respond starting with the letter, then a period, then a brief explanation.\"\n",
        "    )\n",
        "\n",
        "def answer_custom_question(\n",
        "    image: Image.Image | None,\n",
        "    question: str,\n",
        "    choices: list[str],\n",
        "    hint: str | None = None,\n",
        "    lecture: str | None = None,\n",
        "    max_new_tokens: int = 96,\n",
        "):\n",
        "    # Build messages in the same format as during training\n",
        "    user_content = []\n",
        "    if image is not None:\n",
        "        user_content.append({\"type\": \"image\", \"image\": image})\n",
        "    user_content.append(\n",
        "        {\"type\": \"text\", \"text\": build_custom_question_text(question, choices, hint, lecture)}\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]},\n",
        "        {\"role\": \"user\", \"content\": user_content},\n",
        "    ]\n",
        "\n",
        "    text_prompt = ft_processor.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    if image is not None:\n",
        "        image_inputs, _ = process_vision_info(messages)\n",
        "    else:\n",
        "        image_inputs = None\n",
        "\n",
        "    inputs = ft_processor(\n",
        "        text=[text_prompt],\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gen_ids = ft_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "\n",
        "    trimmed = [\n",
        "        out_ids[len(in_ids):]\n",
        "        for in_ids, out_ids in zip(inputs.input_ids, gen_ids)\n",
        "    ]\n",
        "\n",
        "    out_text = ft_processor.batch_decode(\n",
        "        trimmed,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False,\n",
        "    )[0].strip()\n",
        "\n",
        "    letter, expl = parse_letter_and_explanation(out_text)\n",
        "    return letter, expl, out_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa407425-6c60-4109-bfeb-94385b7aa647",
      "metadata": {
        "id": "aa407425-6c60-4109-bfeb-94385b7aa647",
        "outputId": "15443584-a5b0-4a12-88c9-87ecc35999bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted letter: B\n",
            "Explanation: The mouth of an armored catfish is adapted for bottom feeding. It has sharp teeth that can cut through the bottom of a pond or lake.\n",
            "Raw model text: B. The mouth of an armored catfish is adapted for bottom feeding. It has sharp teeth that can cut through the bottom of a pond or lake.\n",
            "Predicted letter: D\n",
            "Explanation: This state is farthest north: Oklahoma.\n",
            "View more states and their locations, and learn about their climates, economies, and cultures.\n"
          ]
        }
      ],
      "source": [
        "# With an image\n",
        "img = val_ds[0][\"image\"]  # or Image.open(\"your_image.png\")\n",
        "q = \"Which animal's mouth is also adapted for bottom feeding?\"\n",
        "choices = [\"discus\", \"armored catfish\"]\n",
        "\n",
        "letter, expl, raw = answer_custom_question(img, q, choices)\n",
        "print(\"Predicted letter:\", letter)\n",
        "print(\"Explanation:\", expl)\n",
        "print(\"Raw model text:\", raw)\n",
        "\n",
        "# Without image\n",
        "letter2, expl2, raw2 = answer_custom_question(\n",
        "    image=None,\n",
        "    question=\"Which state is farthest north?\",\n",
        "    choices=[\"West Virginia\", \"Louisiana\", \"Arizona\", \"Oklahoma\"],\n",
        ")\n",
        "print(\"Predicted letter:\", letter2)\n",
        "print(\"Explanation:\", expl2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f9c4a82-6711-446a-ae55-759199fe1904",
      "metadata": {
        "id": "6f9c4a82-6711-446a-ae55-759199fe1904",
        "outputId": "0143be90-b217-4362-8b04-38969ebacd28"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting summac\n",
            "  Downloading summac-0.0.4-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting detoxify\n",
            "  Downloading detoxify-0.5.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu128)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: transformers>=4.24.0 in /usr/local/lib/python3.12/dist-packages (from summac) (4.57.3)\n",
            "Collecting nltk>=3.6.6 (from summac)\n",
            "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting huggingface-hub<=0.17.0 (from summac)\n",
            "  Downloading huggingface_hub-0.17.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting sentencepiece (from summac)\n",
            "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting protobuf (from summac)\n",
            "  Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<=0.17.0->summac) (3.20.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<=0.17.0->summac) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<=0.17.0->summac) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<=0.17.0->summac) (4.67.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<=0.17.0->summac) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<=0.17.0->summac) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (80.9.0)\n",
            "Collecting click (from nltk>=3.6.6->summac)\n",
            "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting joblib (from nltk>=3.6.6->summac)\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.6.6->summac) (2025.11.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers>=4.24.0 (from summac)\n",
            "  Downloading transformers-4.57.2-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
            "  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n",
            "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
            "  Downloading transformers-4.55.3-py3-none-any.whl.metadata (41 kB)\n",
            "INFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
            "  Downloading transformers-4.55.1-py3-none-any.whl.metadata (41 kB)\n",
            "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
            "  Downloading transformers-4.54.0-py3-none-any.whl.metadata (41 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n",
            "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
            "  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
            "  Downloading transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
            "  Downloading transformers-4.52.2-py3-none-any.whl.metadata (40 kB)\n",
            "  Downloading transformers-4.52.1-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.1-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.44.1-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.43.1-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.43.0-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.42.2-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.42.0-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
            "  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
            "  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
            "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "  Downloading transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n",
            "  Downloading transformers-4.39.1-py3-none-any.whl.metadata (134 kB)\n",
            "  Downloading transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\n",
            "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
            "  Downloading transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
            "  Downloading transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
            "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
            "  Downloading transformers-4.37.1-py3-none-any.whl.metadata (129 kB)\n",
            "  Downloading transformers-4.37.0-py3-none-any.whl.metadata (129 kB)\n",
            "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
            "  Downloading transformers-4.36.1-py3-none-any.whl.metadata (126 kB)\n",
            "  Downloading transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
            "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.24.0->summac)\n",
            "  Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.24.0->summac) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<=0.17.0->summac) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<=0.17.0->summac) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<=0.17.0->summac) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<=0.17.0->summac) (2025.10.5)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading summac-0.0.4-py3-none-any.whl (30 kB)\n",
            "Downloading huggingface_hub-0.17.0-py3-none-any.whl (294 kB)\n",
            "Downloading detoxify-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.3.1-py3-none-any.whl (108 kB)\n",
            "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "Installing collected packages: sentencepiece, protobuf, lightning-utilities, joblib, click, nltk, huggingface-hub, tokenizers, transformers, torchmetrics, summac, detoxify\n",
            "\u001b[2K  Attempting uninstall: huggingface-hub[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [nltk]]]\n",
            "\u001b[2K    Found existing installation: huggingface-hub 0.36.0━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [nltk]\n",
            "\u001b[2K    Uninstalling huggingface-hub-0.36.0:━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [nltk]\n",
            "\u001b[2K      Successfully uninstalled huggingface-hub-0.36.0━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [nltk]\n",
            "\u001b[2K  Attempting uninstall: tokenizers\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [nltk]\n",
            "\u001b[2K    Found existing installation: tokenizers 0.22.1━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [nltk]\n",
            "\u001b[2K    Uninstalling tokenizers-0.22.1:[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [nltk]\n",
            "\u001b[2K      Successfully uninstalled tokenizers-0.22.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [nltk]\n",
            "\u001b[2K  Attempting uninstall: transformers[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/12\u001b[0m [tokenizers]\n",
            "\u001b[2K    Found existing installation: transformers 4.57.3━━━━━━━━━━\u001b[0m \u001b[32m 7/12\u001b[0m [tokenizers]\n",
            "\u001b[2K    Uninstalling transformers-4.57.3:m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/12\u001b[0m [transformers]\n",
            "\u001b[2K      Successfully uninstalled transformers-4.57.3━━━━━━━━━━━━\u001b[0m \u001b[32m 8/12\u001b[0m [transformers]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [detoxify]/12\u001b[0m [torchmetrics]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "accelerate 1.12.0 requires huggingface_hub>=0.21.0, but you have huggingface-hub 0.17.0 which is incompatible.\n",
            "peft 0.18.0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.17.0 which is incompatible.\n",
            "datasets 4.4.1 requires huggingface-hub<2.0,>=0.25.0, but you have huggingface-hub 0.17.0 which is incompatible.\n",
            "trl 0.25.1 requires transformers>=4.56.1, but you have transformers 4.35.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed click-8.3.1 detoxify-0.5.2 huggingface-hub-0.17.0 joblib-1.5.2 lightning-utilities-0.15.2 nltk-3.9.2 protobuf-6.33.1 sentencepiece-0.2.1 summac-0.0.4 tokenizers-0.15.2 torchmetrics-1.8.2 transformers-4.35.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics summac detoxify\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7998b672-699c-4a89-a9ca-59257c651f5e",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "85eac7df15b440feb1c70193f507668f"
          ]
        },
        "id": "7998b672-699c-4a89-a9ca-59257c651f5e",
        "outputId": "061328f6-2500-4619-e925-cb6735479d27"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85eac7df15b440feb1c70193f507668f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "BASE_MODEL_ID = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "\n",
        "base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.bfloat16,\n",
        ")\n",
        "base_model.eval()\n",
        "\n",
        "base_processor = Qwen2VLProcessor.from_pretrained(BASE_MODEL_ID)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b8ea55-8347-47a0-bd84-489cd0e1de49",
      "metadata": {
        "id": "f3b8ea55-8347-47a0-bd84-489cd0e1de49"
      },
      "outputs": [],
      "source": [
        "def generate_answer_with_explanation_generic(model, processor, example, max_new_tokens=64):\n",
        "    \"\"\"\n",
        "    Uses the same formatting as training (system + user) to generate answer+explanation.\n",
        "    \"\"\"\n",
        "    formatted = format_example(example)          # reuses your training formatter\n",
        "    conv_for_gen = formatted[\"messages\"][:2]     # system + user only\n",
        "\n",
        "    text_prompt = processor.apply_chat_template(\n",
        "        conv_for_gen,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    image_inputs, _ = process_vision_info(formatted[\"messages\"])\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text_prompt],\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gen_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "\n",
        "    trimmed = [\n",
        "        out_ids[len(in_ids):]\n",
        "        for in_ids, out_ids in zip(inputs.input_ids, gen_ids)\n",
        "    ]\n",
        "\n",
        "    out_text = processor.batch_decode(\n",
        "        trimmed,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False,\n",
        "    )[0].strip()\n",
        "\n",
        "    return out_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b727da39-ddde-4f36-9777-dae2c6814442",
      "metadata": {
        "id": "b727da39-ddde-4f36-9777-dae2c6814442"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def parse_letter_and_explanation(text):\n",
        "    m = re.match(r\"\\s*([A-Z])\\s*[\\.\\-\\):]\\s*(.*)\", text, flags=re.DOTALL)\n",
        "    if m:\n",
        "        letter = m.group(1)\n",
        "        explanation = m.group(2).strip()\n",
        "    else:\n",
        "        m2 = re.search(r\"[A-Z]\", text)\n",
        "        if m2:\n",
        "            letter = m2.group(0)\n",
        "            explanation = text[m2.end():].strip()\n",
        "        else:\n",
        "            letter = None\n",
        "            explanation = text.strip()\n",
        "    return letter, explanation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62005efd-1f7d-4966-aba5-892fe80a50bb",
      "metadata": {
        "id": "62005efd-1f7d-4966-aba5-892fe80a50bb",
        "outputId": "3cb527fc-e899-48c2-ae8d-71654093978c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Collecting outputs: 100%|██████████| 500/500 [09:49<00:00,  1.18s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def collect_outputs(dataset, num_samples=None, max_new_tokens=64):\n",
        "    \"\"\"\n",
        "    Collect model outputs + metadata for both base and fine-tuned models.\n",
        "    \"\"\"\n",
        "    n = len(dataset) if num_samples is None else min(num_samples, len(dataset))\n",
        "    records = []\n",
        "\n",
        "    for i in tqdm(range(n), desc=\"Collecting outputs\"):\n",
        "        ex = dataset[i]\n",
        "        true_letter = index_to_letter(int(ex[\"answer\"]))\n",
        "        solution = ex.get(\"solution\", \"\")\n",
        "\n",
        "        # Base\n",
        "        base_raw = generate_answer_with_explanation_generic(\n",
        "            base_model, base_processor, ex, max_new_tokens=max_new_tokens\n",
        "        )\n",
        "        base_letter, base_expl = parse_letter_and_explanation(base_raw)\n",
        "\n",
        "        # Finetuned\n",
        "        ft_raw = generate_answer_with_explanation_generic(\n",
        "            ft_model, ft_processor, ex, max_new_tokens=max_new_tokens\n",
        "        )\n",
        "        ft_letter, ft_expl = parse_letter_and_explanation(ft_raw)\n",
        "\n",
        "        # Source text we’ll use for hallucination metrics\n",
        "        source_text = build_question_text(ex) + \"\\n\\n\" + (solution or \"\")\n",
        "\n",
        "        records.append(\n",
        "            {\n",
        "                \"example\": ex,\n",
        "                \"true_letter\": true_letter,\n",
        "                \"solution\": solution,\n",
        "                \"source_text\": source_text,\n",
        "\n",
        "                \"base_raw\": base_raw,\n",
        "                \"base_letter\": base_letter,\n",
        "                \"base_expl\": base_expl,\n",
        "\n",
        "                \"ft_raw\": ft_raw,\n",
        "                \"ft_letter\": ft_letter,\n",
        "                \"ft_expl\": ft_expl,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return records\n",
        "\n",
        "# Example: collect on first 500 validation examples\n",
        "val_records = collect_outputs(val_ds, num_samples=500, max_new_tokens=96)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "076a3fad-7ef5-4b48-9fe9-21fcaa897382",
      "metadata": {
        "id": "076a3fad-7ef5-4b48-9fe9-21fcaa897382",
        "outputId": "a5d80b67-54a3-4589-f7a5-69f467a6823f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base VQA: {'VQA_EM': 0.652, 'VQA_F1_explanation': 0.20842234513052912}\n",
            "FT   VQA: {'VQA_EM': 0.696, 'VQA_F1_explanation': 0.7628293064480157}\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "def _token_f1(pred, gold):\n",
        "    pred_tokens = pred.lower().split()\n",
        "    gold_tokens = gold.lower().split()\n",
        "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    common = Counter(pred_tokens) & Counter(gold_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0.0\n",
        "\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(gold_tokens)\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "def vqa_metrics(records, model_prefix=\"base\"):\n",
        "    em_list = []\n",
        "    f1_list = []\n",
        "\n",
        "    for r in records:\n",
        "        true_letter = r[\"true_letter\"]\n",
        "        sol = r[\"solution\"] or \"\"\n",
        "\n",
        "        pred_letter = r[f\"{model_prefix}_letter\"]\n",
        "        pred_expl   = r[f\"{model_prefix}_expl\"] or \"\"\n",
        "\n",
        "        em_list.append(1.0 if pred_letter == true_letter else 0.0)\n",
        "        if sol:\n",
        "            f1_list.append(_token_f1(pred_expl, sol))\n",
        "        else:\n",
        "            # If no solution text, ignore in F1\n",
        "            pass\n",
        "\n",
        "    em = float(np.mean(em_list))\n",
        "    f1 = float(np.mean(f1_list)) if f1_list else 0.0\n",
        "    return {\"VQA_EM\": em, \"VQA_F1_explanation\": f1}\n",
        "\n",
        "base_vqa = vqa_metrics(val_records, \"base\")\n",
        "ft_vqa   = vqa_metrics(val_records, \"ft\")\n",
        "\n",
        "print(\"Base VQA:\", base_vqa)\n",
        "print(\"FT   VQA:\", ft_vqa)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc177b65-156b-43a0-9c09-3dbdef9cb480",
      "metadata": {
        "id": "fc177b65-156b-43a0-9c09-3dbdef9cb480",
        "outputId": "530df929-8e36-4235-8e14-b3b37206253a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu128)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.1.2)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.8.0+cu128)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchvision) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134427a5-343d-4423-9ab3-f00293e791f9",
      "metadata": {
        "id": "134427a5-343d-4423-9ab3-f00293e791f9",
        "outputId": "2a5b4a53-ef00-4f2c-eace-5b75e1fc5f99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (98 > 77). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: Encountered caption longer than max_position_embeddings=77. Will truncate captions to thislength. If longer captions are needed, initialize argument `model_name_or_path` with a model thatsupports longer sequences.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base CLIP: {'CLIPScore': 23.061771392822266}\n",
            "FT   CLIP: {'CLIPScore': 22.325023651123047}\n"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchmetrics.multimodal import CLIPScore\n",
        "\n",
        "# create metric once\n",
        "clip_metric = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\").to(device)\n",
        "to_tensor = transforms.ToTensor()\n",
        "\n",
        "def clipscore_metric(records, model_prefix=\"base\"):\n",
        "    \"\"\"\n",
        "    Compute CLIPScore over a batch of (image, text) pairs.\n",
        "\n",
        "    - Converts PIL images to torch tensors (C, H, W)\n",
        "    - Calls CLIPScore once over lists of images/texts\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    texts = []\n",
        "\n",
        "    for r in records:\n",
        "        img_pil = r[\"example\"][\"image\"]       # PIL.Image\n",
        "        text = r[f\"{model_prefix}_raw\"]       # string\n",
        "\n",
        "        img_tensor = to_tensor(img_pil)       # (C, H, W), float32 in [0,1]\n",
        "        images.append(img_tensor)\n",
        "        texts.append(text)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # CLIPScore can take List[Tensor] + List[str]\n",
        "        score = clip_metric(images, texts)\n",
        "\n",
        "    return {\"CLIPScore\": float(score.item())}\n",
        "\n",
        "# now this should work:\n",
        "base_clip = clipscore_metric(val_records, \"base\")\n",
        "ft_clip   = clipscore_metric(val_records, \"ft\")\n",
        "\n",
        "print(\"Base CLIP:\", base_clip)\n",
        "print(\"FT   CLIP:\", ft_clip)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1108eedb-9fe1-430d-a26c-0b8e63a157e0",
      "metadata": {
        "id": "1108eedb-9fe1-430d-a26c-0b8e63a157e0"
      },
      "outputs": [],
      "source": [
        "from summac.model import SummaCZS\n",
        "\n",
        "summac_model = SummaCZS(granularity=\"sentence\", model_name=\"vitc\")\n",
        "\n",
        "def summac_metric(records, model_prefix=\"base\"):\n",
        "    sources = [r[\"source_text\"] for r in records]\n",
        "    summaries = [r[f\"{model_prefix}_expl\"] or \"\" for r in records]\n",
        "\n",
        "    scores = summac_model.score(\n",
        "        sources=sources,\n",
        "        summaries=summaries,\n",
        "    )[\"scores\"]\n",
        "\n",
        "    return {\"SummaC_mean\": float(np.mean(scores))}\n",
        "\n",
        "base_summac = summac_metric(val_records, \"base\")\n",
        "ft_summac   = summac_metric(val_records, \"ft\")\n",
        "\n",
        "print(\"Base SummaC:\", base_summac)\n",
        "print(\"FT   SummaC:\", ft_summac)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fae06ec-fec7-41ba-bdd5-834cffe54053",
      "metadata": {
        "id": "9fae06ec-fec7-41ba-bdd5-834cffe54053"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch.nn.functional as F\n",
        "\n",
        "factcc_model_id = \"manueldeprada/FactCC\"\n",
        "factcc_tokenizer = AutoTokenizer.from_pretrained(factcc_model_id)\n",
        "factcc_model = AutoModelForSequenceClassification.from_pretrained(factcc_model_id).to(device)\n",
        "factcc_model.eval()\n",
        "\n",
        "def factcc_metric(records, model_prefix=\"base\", batch_size=16):\n",
        "    probs_all = []\n",
        "    for i in range(0, len(records), batch_size):\n",
        "        batch = records[i:i+batch_size]\n",
        "        sources = [r[\"source_text\"] for r in batch]\n",
        "        summaries = [r[f\"{model_prefix}_expl\"] or \"\" for r in batch]\n",
        "\n",
        "        enc = factcc_tokenizer(\n",
        "            sources,\n",
        "            summaries,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = factcc_model(**enc).logits\n",
        "            probs = F.softmax(logits, dim=-1)[:, 1]  # assume label 1 = factual\n",
        "        probs_all.extend(probs.cpu().tolist())\n",
        "\n",
        "    return {\n",
        "        \"FactCC_mean_prob_factual\": float(np.mean(probs_all))\n",
        "    }\n",
        "\n",
        "base_factcc = factcc_metric(val_records, \"base\")\n",
        "ft_factcc   = factcc_metric(val_records, \"ft\")\n",
        "\n",
        "print(\"Base FactCC:\", base_factcc)\n",
        "print(\"FT   FactCC:\", ft_factcc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb985bf2-84e3-4505-9e41-ba4c73e127d8",
      "metadata": {
        "id": "cb985bf2-84e3-4505-9e41-ba4c73e127d8"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()   # assumes OPENAI_API_KEY env var\n",
        "\n",
        "GEVAL_PROMPT = \"\"\"\n",
        "You are grading answers to science visual question answering problems.\n",
        "\n",
        "You will receive:\n",
        "- the question and answer choices\n",
        "- a reference correct answer letter\n",
        "- the model's predicted letter and explanation\n",
        "\n",
        "For each output, give three scores from 1 to 5:\n",
        "- Fluency: Is the explanation clear, grammatical and well-written?\n",
        "- Relevance: Does the explanation focus on the question and image, rather than irrelevant details?\n",
        "- Correctness: Is the predicted answer letter correct and is the explanation logically consistent with the source?\n",
        "\n",
        "Respond in JSON with keys: fluency, relevance, correctness.\n",
        "\"\"\"\n",
        "\n",
        "def geval_score_one(record, model_prefix=\"base\"):\n",
        "    ex = record[\"example\"]\n",
        "    q_block = build_question_text(ex)\n",
        "    ref_letter = record[\"true_letter\"]\n",
        "\n",
        "    pred_raw = record[f\"{model_prefix}_raw\"]\n",
        "\n",
        "    user_content = f\"\"\"\n",
        "QUESTION & CONTEXT:\n",
        "{q_block}\n",
        "\n",
        "REFERENCE ANSWER LETTER: {ref_letter}\n",
        "\n",
        "MODEL OUTPUT:\n",
        "{pred_raw}\n",
        "\"\"\"\n",
        "\n",
        "    resp = client.responses.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        input=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": GEVAL_PROMPT,\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_content,\n",
        "            },\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    text = resp.output[0].content[0].text\n",
        "    # parse JSON from `text`\n",
        "    import json\n",
        "    scores = json.loads(text)\n",
        "    return scores[\"fluency\"], scores[\"relevance\"], scores[\"correctness\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2905fc3c-e562-40f8-ba49-dfd1b3086f33",
      "metadata": {
        "id": "2905fc3c-e562-40f8-ba49-dfd1b3086f33"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a0c44e6-00a7-4475-98a2-b05ab11d249a",
      "metadata": {
        "id": "0a0c44e6-00a7-4475-98a2-b05ab11d249a"
      },
      "outputs": [],
      "source": [
        "def geval_metric(records, model_prefix=\"base\", num_samples=100):\n",
        "    fl, rel, corr = [], [], []\n",
        "    for r in records[:num_samples]:\n",
        "        f, r_, c = geval_score_one(r, model_prefix)\n",
        "        fl.append(f); rel.append(r_); corr.append(c)\n",
        "    return {\n",
        "        \"G_eval_fluency\":   float(np.mean(fl)),\n",
        "        \"G_eval_relevance\": float(np.mean(rel)),\n",
        "        \"G_eval_correctness\": float(np.mean(corr)),\n",
        "    }\n",
        "\n",
        "base_geval = geval_metric(val_records, \"base\", num_samples=100)\n",
        "ft_geval   = geval_metric(val_records, \"ft\", num_samples=100)\n",
        "\n",
        "print(\"Base G-Eval:\", base_geval)\n",
        "print(\"FT   G-Eval:\", ft_geval)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd6b5152-46c8-42f2-af74-f0de8aa69be4",
      "metadata": {
        "id": "fd6b5152-46c8-42f2-af74-f0de8aa69be4"
      },
      "outputs": [],
      "source": [
        "from detoxify import Detoxify\n",
        "\n",
        "tox_model = Detoxify('original')  # or 'unbiased'\n",
        "\n",
        "def toxicity_metric(records, model_prefix=\"base\"):\n",
        "    texts = [r[f\"{model_prefix}_raw\"] for r in records]\n",
        "    scores = tox_model.predict(texts)[\"toxicity\"]   # np.array\n",
        "    return {\n",
        "        \"toxicity_mean\": float(np.mean(scores)),\n",
        "        \"toxicity_95p\": float(np.percentile(scores, 95)),\n",
        "    }\n",
        "\n",
        "base_tox = toxicity_metric(val_records, \"base\")\n",
        "ft_tox   = toxicity_metric(val_records, \"ft\")\n",
        "\n",
        "print(\"Base toxicity:\", base_tox)\n",
        "print(\"FT   toxicity:\", ft_tox)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b1c022-95a9-4750-8dfc-a87839d5f088",
      "metadata": {
        "id": "57b1c022-95a9-4750-8dfc-a87839d5f088"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nsfw_pipe = pipeline(\n",
        "    \"image-classification\",\n",
        "    model=\"Falconsai/nsfw_image_detection\",\n",
        "    device=0 if device == \"cuda\" else -1,\n",
        ")\n",
        "\n",
        "def nsfw_metric(records, threshold=0.5):\n",
        "    nsfw_probs = []\n",
        "    for r in records:\n",
        "        img = r[\"example\"][\"image\"]\n",
        "        preds = nsfw_pipe(img)\n",
        "        # assumes label 'nsfw' exists\n",
        "        prob_nsfw = next(p[\"score\"] for p in preds if \"nsfw\" in p[\"label\"].lower())\n",
        "        nsfw_probs.append(prob_nsfw)\n",
        "\n",
        "    return {\n",
        "        \"NSFW_mean\": float(np.mean(nsfw_probs)),\n",
        "        \"NSFW_95p\": float(np.percentile(nsfw_probs, 95)),\n",
        "    }\n",
        "\n",
        "images_nsfw = nsfw_metric(val_records)\n",
        "print(\"Images NSFW stats:\", images_nsfw)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccb48961-f861-46b3-b206-bbd0a8c4ceb1",
      "metadata": {
        "id": "ccb48961-f861-46b3-b206-bbd0a8c4ceb1"
      },
      "outputs": [],
      "source": [
        "def merge_metrics(*metric_dicts):\n",
        "    merged = {}\n",
        "    for d in metric_dicts:\n",
        "        merged.update(d)\n",
        "    return merged\n",
        "\n",
        "base_all = merge_metrics(base_vqa, base_clip, base_summac, base_factcc)\n",
        "ft_all   = merge_metrics(ft_vqa,   ft_clip,  ft_summac,  ft_factcc)\n",
        "\n",
        "print(\"\\n=== Metric comparison (validation, ~500 examples) ===\")\n",
        "for k in sorted(base_all.keys()):\n",
        "    print(f\"{k:30s}  base={base_all[k]:.4f}   ft={ft_all[k]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cde73999-32df-4164-befc-bf3161356d67",
      "metadata": {
        "id": "cde73999-32df-4164-befc-bf3161356d67"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY=\"OPENAI_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e59a9b10-7ef0-4954-98a4-66c139861863",
      "metadata": {
        "id": "e59a9b10-7ef0-4954-98a4-66c139861863"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "def load_openai_client_from_variable(key):\n",
        "    if not key:\n",
        "        print(\"No API key provided — skipping LLM-as-judge scoring.\")\n",
        "        return None\n",
        "    return OpenAI(api_key=key)\n",
        "\n",
        "client = load_openai_client_from_variable(OPENAI_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9acc8760-034e-4492-aa89-46a2b27b9044",
      "metadata": {
        "id": "9acc8760-034e-4492-aa89-46a2b27b9044",
        "outputId": "c727e295-b16e-473a-e7bd-78ed44693efb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: summac in /usr/local/lib/python3.12/dist-packages (0.0.4)\n",
            "Requirement already satisfied: detoxify in /usr/local/lib/python3.12/dist-packages (0.5.2)\n",
            "Collecting openai\n",
            "  Downloading openai-2.9.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu128)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu128)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: transformers>=4.24.0 in /usr/local/lib/python3.12/dist-packages (from summac) (4.35.2)\n",
            "Requirement already satisfied: nltk>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from summac) (3.9.2)\n",
            "Requirement already satisfied: huggingface-hub<=0.17.0 in /usr/local/lib/python3.12/dist-packages (from summac) (0.17.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from summac) (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from summac) (6.33.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<=0.17.0->summac) (3.20.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<=0.17.0->summac) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<=0.17.0->summac) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<=0.17.0->summac) (4.67.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<=0.17.0->summac) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<=0.17.0->summac) (4.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Collecting jiter<1,>=0.10.0 (from openai)\n",
            "  Downloading jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai)\n",
            "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai)\n",
            "  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai)\n",
            "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.6.6->summac) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.6.6->summac) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.6.6->summac) (2025.11.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.24.0->summac) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.24.0->summac) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<=0.17.0->summac) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<=0.17.0->summac) (2.5.0)\n",
            "Downloading openai-2.9.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (361 kB)\n",
            "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
            "Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: typing-inspection, pydantic-core, jiter, annotated-types, pydantic, openai\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [openai]2m5/6\u001b[0m [openai]c]\n",
            "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 jiter-0.12.0 openai-2.9.0 pydantic-2.12.5 pydantic-core-2.41.5 typing-inspection-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics summac detoxify openai torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfcbdbda-ae0e-4f5f-9011-4abb0138416f",
      "metadata": {
        "id": "dfcbdbda-ae0e-4f5f-9011-4abb0138416f",
        "outputId": "5476d37e-3cd5-4155-ad13-d8016c0d17ec"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'summac.model'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmultimodal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CLIPScore\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msummac\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SummaCZS\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdetoxify\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Detoxify\n\u001b[32m     28\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'summac.model'"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# ONE-CELL EVAL: base vs finetuned Qwen2-VL\n",
        "# =========================================\n",
        "# If needed, install once (then comment out):\n",
        "# !pip install torchmetrics summac detoxify openai torchvision\n",
        "\n",
        "import os, json, re\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import (\n",
        "    Qwen2VLForConditionalGeneration,\n",
        "    Qwen2VLProcessor,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    pipeline,\n",
        ")\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from torchmetrics.multimodal import CLIPScore\n",
        "from torchvision import transforms\n",
        "from summac.model import SummaCZS\n",
        "from detoxify import Detoxify\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------------\n",
        "# 0. Dataset-dependent helpers\n",
        "# -----------------------------\n",
        "# Re-declare these to match training (safe to override)\n",
        "\n",
        "system_message = \"\"\"You are a helpful science tutor.\n",
        "You see a question, several answer choices, and often an image and a hint/lecture.\n",
        "Your job is to pick the single correct multiple-choice option.\n",
        "Start your answer with ONLY the letter of the correct option (A, B, C, D, etc.),\n",
        "then a period, then a short explanation.\n",
        "Example: \"C. Because ...\"\n",
        "\"\"\"\n",
        "\n",
        "LETTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "\n",
        "def index_to_letter(idx: int) -> str:\n",
        "    return LETTERS[idx]\n",
        "\n",
        "def build_question_text(example) -> str:\n",
        "    choices = example[\"choices\"]\n",
        "    options_text = \"\\n\".join(\n",
        "        f\"({LETTERS[i]}) {choice}\" for i, choice in enumerate(choices)\n",
        "    )\n",
        "    hint = example.get(\"hint\", \"\")\n",
        "    lecture = example.get(\"lecture\", \"\")\n",
        "    hint_part = f\"\\nHint: {hint}\" if hint else \"\"\n",
        "    lecture_part = f\"\\nLecture: {lecture}\" if lecture else \"\"\n",
        "    return (\n",
        "        f\"Question: {example['question']}\\n\\n\"\n",
        "        f\"Choices:\\n{options_text}\"\n",
        "        f\"{hint_part}{lecture_part}\\n\\n\"\n",
        "        f\"Respond starting with the letter, then a period, then a brief explanation.\"\n",
        "    )\n",
        "\n",
        "def format_example(example):\n",
        "    correct_letter = index_to_letter(int(example[\"answer\"]))\n",
        "    explanation = example.get(\"solution\", \"\") or example.get(\"explanation\", \"\")\n",
        "    if explanation:\n",
        "        assistant_text = f\"{correct_letter}. {explanation}\"\n",
        "    else:\n",
        "        assistant_text = correct_letter\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": example[\"image\"]},\n",
        "                {\"type\": \"text\", \"text\": build_question_text(example)},\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": assistant_text}],\n",
        "        },\n",
        "    ]\n",
        "    return {\"images\": [example[\"image\"]], \"messages\": messages}\n",
        "\n",
        "def parse_letter_and_explanation(text: str):\n",
        "    m = re.match(r\"\\s*([A-Z])\\s*[\\.\\-\\):]\\s*(.*)\", text, flags=re.DOTALL)\n",
        "    if m:\n",
        "        letter = m.group(1)\n",
        "        explanation = m.group(2).strip()\n",
        "    else:\n",
        "        m2 = re.search(r\"[A-Z]\", text)\n",
        "        if m2:\n",
        "            letter = m2.group(0)\n",
        "            explanation = text[m2.end():].strip()\n",
        "        else:\n",
        "            letter = None\n",
        "            explanation = text.strip()\n",
        "    return letter, explanation\n",
        "\n",
        "# --------------------------------\n",
        "# 1. Load base & finetuned models\n",
        "# --------------------------------\n",
        "BASE_MODEL_ID = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "FT_ADAPTER_DIR = \"qwen2-vl-2b-scienceqa-lora-expl\"  # change if you used another dir\n",
        "\n",
        "print(\"Loading base model...\")\n",
        "base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "base_model.eval()\n",
        "base_processor = Qwen2VLProcessor.from_pretrained(BASE_MODEL_ID)\n",
        "\n",
        "print(\"Loading finetuned model + adapter...\")\n",
        "ft_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "ft_model.load_adapter(FT_ADAPTER_DIR)\n",
        "ft_model.eval()\n",
        "ft_processor = Qwen2VLProcessor.from_pretrained(BASE_MODEL_ID)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Shared generation helper\n",
        "# -----------------------------\n",
        "def generate_answer_with_explanation_generic(model, processor, example, max_new_tokens=96):\n",
        "    formatted = format_example(example)\n",
        "    conv_for_gen = formatted[\"messages\"][:2]  # system + user\n",
        "\n",
        "    text_prompt = processor.apply_chat_template(\n",
        "        conv_for_gen,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    image_inputs, _ = process_vision_info(formatted[\"messages\"])\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text_prompt],\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gen_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "\n",
        "    trimmed = [\n",
        "        out_ids[len(in_ids):]\n",
        "        for in_ids, out_ids in zip(inputs.input_ids, gen_ids)\n",
        "    ]\n",
        "\n",
        "    out_text = processor.batch_decode(\n",
        "        trimmed,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False,\n",
        "    )[0].strip()\n",
        "\n",
        "    return out_text\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3. Collect outputs (base & finetuned)\n",
        "# ----------------------------------------\n",
        "def collect_outputs(dataset, num_samples=None, max_new_tokens=96):\n",
        "    n = len(dataset) if num_samples is None else min(num_samples, len(dataset))\n",
        "    records = []\n",
        "    for i in tqdm(range(n), desc=\"Collecting outputs\"):\n",
        "        ex = dataset[i]\n",
        "        true_letter = index_to_letter(int(ex[\"answer\"]))\n",
        "        solution = ex.get(\"solution\", \"\")\n",
        "\n",
        "        base_raw = generate_answer_with_explanation_generic(\n",
        "            base_model, base_processor, ex, max_new_tokens=max_new_tokens\n",
        "        )\n",
        "        base_letter, base_expl = parse_letter_and_explanation(base_raw)\n",
        "\n",
        "        ft_raw = generate_answer_with_explanation_generic(\n",
        "            ft_model, ft_processor, ex, max_new_tokens=max_new_tokens\n",
        "        )\n",
        "        ft_letter, ft_expl = parse_letter_and_explanation(ft_raw)\n",
        "\n",
        "        source_text = build_question_text(ex) + \"\\n\\n\" + (solution or \"\")\n",
        "\n",
        "        records.append(\n",
        "            {\n",
        "                \"example\": ex,\n",
        "                \"true_letter\": true_letter,\n",
        "                \"solution\": solution,\n",
        "                \"source_text\": source_text,\n",
        "                \"base_raw\": base_raw,\n",
        "                \"base_letter\": base_letter,\n",
        "                \"base_expl\": base_expl,\n",
        "                \"ft_raw\": ft_raw,\n",
        "                \"ft_letter\": ft_letter,\n",
        "                \"ft_expl\": ft_expl,\n",
        "            }\n",
        "        )\n",
        "    return records\n",
        "\n",
        "# Choose number of validation examples to evaluate\n",
        "NUM_VAL_EXAMPLES = 300   # adjust as you like\n",
        "print(f\"Collecting outputs on {NUM_VAL_EXAMPLES} val examples...\")\n",
        "val_records = collect_outputs(val_ds, num_samples=NUM_VAL_EXAMPLES, max_new_tokens=96)\n",
        "\n",
        "# --------------------------------\n",
        "# 4. Metric 1 – VQA EM / F1\n",
        "# --------------------------------\n",
        "def _token_f1(pred, gold):\n",
        "    pred_tokens = pred.lower().split()\n",
        "    gold_tokens = gold.lower().split()\n",
        "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "        return 0.0\n",
        "    common = Counter(pred_tokens) & Counter(gold_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0.0\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(gold_tokens)\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "def vqa_metrics(records, model_prefix=\"base\"):\n",
        "    em_list, f1_list = [], []\n",
        "    for r in records:\n",
        "        true_letter = r[\"true_letter\"]\n",
        "        sol = r[\"solution\"] or \"\"\n",
        "        pred_letter = r[f\"{model_prefix}_letter\"]\n",
        "        pred_expl = r[f\"{model_prefix}_expl\"] or \"\"\n",
        "        em_list.append(1.0 if pred_letter == true_letter else 0.0)\n",
        "        if sol:\n",
        "            f1_list.append(_token_f1(pred_expl, sol))\n",
        "    em = float(np.mean(em_list))\n",
        "    f1 = float(np.mean(f1_list)) if f1_list else 0.0\n",
        "    return {\"VQA_EM\": em, \"VQA_F1_expl\": f1}\n",
        "\n",
        "# --------------------------------\n",
        "# 5. Metric 2 – CLIPScore\n",
        "# --------------------------------\n",
        "clip_metric = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\").to(device)\n",
        "to_tensor = transforms.ToTensor()\n",
        "\n",
        "def clipscore_metric(records, model_prefix=\"base\"):\n",
        "    images, texts = [], []\n",
        "    for r in records:\n",
        "        img_pil = r[\"example\"][\"image\"]\n",
        "        text = r[f\"{model_prefix}_raw\"]\n",
        "        img_tensor = to_tensor(img_pil)  # (C, H, W)\n",
        "        images.append(img_tensor)\n",
        "        texts.append(text)\n",
        "    with torch.no_grad():\n",
        "        score = clip_metric(images, texts)\n",
        "    return {\"CLIPScore\": float(score.item())}\n",
        "\n",
        "# ----------------------------------------------\n",
        "# 6. Metric 3 – SummaC & FactCC (hallucination)\n",
        "# ----------------------------------------------\n",
        "print(\"Loading SummaC model...\")\n",
        "summac_model = SummaCZS(granularity=\"sentence\", model_name=\"vitc\")\n",
        "\n",
        "def summac_metric(records, model_prefix=\"base\"):\n",
        "    sources = [r[\"source_text\"] for r in records]\n",
        "    summaries = [r[f\"{model_prefix}_expl\"] or \"\" for r in records]\n",
        "    scores = summac_model.score(sources=sources, summaries=summaries)[\"scores\"]\n",
        "    return {\"SummaC_mean\": float(np.mean(scores))}\n",
        "\n",
        "print(\"Loading FactCC model...\")\n",
        "factcc_model_id = \"manueldeprada/FactCC\"  # change if needed\n",
        "factcc_tokenizer = AutoTokenizer.from_pretrained(factcc_model_id)\n",
        "factcc_model = AutoModelForSequenceClassification.from_pretrained(factcc_model_id).to(device)\n",
        "factcc_model.eval()\n",
        "\n",
        "def factcc_metric(records, model_prefix=\"base\", batch_size=16):\n",
        "    probs_all = []\n",
        "    for i in range(0, len(records), batch_size):\n",
        "        batch = records[i:i+batch_size]\n",
        "        sources = [r[\"source_text\"] for r in batch]\n",
        "        summaries = [r[f\"{model_prefix}_expl\"] or \"\" for r in batch]\n",
        "        enc = factcc_tokenizer(\n",
        "            sources,\n",
        "            summaries,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = factcc_model(**enc).logits\n",
        "            # assume label 1 = factual\n",
        "            probs = torch.softmax(logits, dim=-1)[:, 1]\n",
        "        probs_all.extend(probs.cpu().tolist())\n",
        "    return {\"FactCC_mean_prob_factual\": float(np.mean(probs_all))}\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 7. Metric 4 – G-Eval (LLM-as-judge) [optional]\n",
        "# ---------------------------------------------------\n",
        "def maybe_load_openai_client():\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "        if os.getenv(\"OPENAI_API_KEY\"):\n",
        "            return OpenAI()\n",
        "    except ImportError:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "client = maybe_load_openai_client()\n",
        "\n",
        "GEVAL_SYSTEM_PROMPT = \"\"\"\n",
        "You are grading answers to science visual question answering problems.\n",
        "\n",
        "You will receive:\n",
        "- the question and answer choices\n",
        "- the reference correct answer letter\n",
        "- the model's predicted answer and explanation\n",
        "\n",
        "For each output, give three scores from 1 to 5:\n",
        "- Fluency: Is the explanation clear, grammatical and well-written?\n",
        "- Relevance: Does the explanation focus on the question and image/context?\n",
        "- Correctness: Is the predicted answer letter correct and is the explanation logically consistent with the context?\n",
        "\n",
        "Respond ONLY in JSON with keys: fluency, relevance, correctness.\n",
        "\"\"\"\n",
        "\n",
        "def geval_score_one(record, model_prefix=\"base\"):\n",
        "    ex = record[\"example\"]\n",
        "    q_block = build_question_text(ex)\n",
        "    ref_letter = record[\"true_letter\"]\n",
        "    pred_raw = record[f\"{model_prefix}_raw\"]\n",
        "\n",
        "    user_content = f\"\"\"\n",
        "QUESTION & CONTEXT:\n",
        "{q_block}\n",
        "\n",
        "REFERENCE ANSWER LETTER: {ref_letter}\n",
        "\n",
        "MODEL OUTPUT:\n",
        "{pred_raw}\n",
        "\"\"\"\n",
        "\n",
        "    resp = client.responses.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        input=[\n",
        "            {\"role\": \"system\", \"content\": GEVAL_SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "        ],\n",
        "    )\n",
        "    text = resp.output[0].content[0].text\n",
        "    scores = json.loads(text)\n",
        "    return scores[\"fluency\"], scores[\"relevance\"], scores[\"correctness\"]\n",
        "\n",
        "def geval_metric(records, model_prefix=\"base\", num_samples=50):\n",
        "    if client is None:\n",
        "        print(\"⚠️ Skipping G-Eval: OpenAI client or API key not available.\")\n",
        "        return {}\n",
        "    fl, rel, corr = [], [], []\n",
        "    for r in records[:num_samples]:\n",
        "        f, r_, c = geval_score_one(r, model_prefix)\n",
        "        fl.append(f); rel.append(r_); corr.append(c)\n",
        "    return {\n",
        "        \"G_eval_fluency\": float(np.mean(fl)),\n",
        "        \"G_eval_relevance\": float(np.mean(rel)),\n",
        "        \"G_eval_correctness\": float(np.mean(corr)),\n",
        "    }\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 8. Metric 5 – Safety: toxicity + NSFW image detection\n",
        "# ---------------------------------------------------\n",
        "print(\"Loading Detoxify toxicity model...\")\n",
        "tox_model = Detoxify(\"unbiased\")\n",
        "\n",
        "def toxicity_metric(records, model_prefix=\"base\"):\n",
        "    texts = [r[f\"{model_prefix}_raw\"] for r in records]\n",
        "    scores = tox_model.predict(texts)[\"toxicity\"]  # numpy array\n",
        "    return {\n",
        "        \"toxicity_mean\": float(np.mean(scores)),\n",
        "        \"toxicity_95p\": float(np.percentile(scores, 95)),\n",
        "    }\n",
        "\n",
        "print(\"Loading NSFW image detection pipeline...\")\n",
        "nsfw_pipe = pipeline(\n",
        "    \"image-classification\",\n",
        "    model=\"Falconsai/nsfw_image_detection\",\n",
        "    device=0 if device.type == \"cuda\" else -1,\n",
        ")\n",
        "\n",
        "def nsfw_metric(records, threshold=0.5):\n",
        "    nsfw_probs = []\n",
        "    for r in records:\n",
        "        img = r[\"example\"][\"image\"]\n",
        "        preds = nsfw_pipe(img)\n",
        "        prob_nsfw = 0.0\n",
        "        for p in preds:\n",
        "            if \"nsfw\" in p[\"label\"].lower():\n",
        "                prob_nsfw = p[\"score\"]\n",
        "                break\n",
        "        nsfw_probs.append(prob_nsfw)\n",
        "    return {\n",
        "        \"NSFW_mean\": float(np.mean(nsfw_probs)),\n",
        "        \"NSFW_95p\": float(np.percentile(nsfw_probs, 95)),\n",
        "    }\n",
        "\n",
        "# -------------------------------------------\n",
        "# 9. Compute ALL metrics & print comparison\n",
        "# -------------------------------------------\n",
        "def merge_metrics(*metric_dicts):\n",
        "    merged = {}\n",
        "    for d in metric_dicts:\n",
        "        merged.update(d)\n",
        "    return merged\n",
        "\n",
        "print(\"\\nComputing metrics...\")\n",
        "\n",
        "base_vqa   = vqa_metrics(val_records, \"base\")\n",
        "ft_vqa     = vqa_metrics(val_records, \"ft\")\n",
        "\n",
        "base_clip  = clipscore_metric(val_records, \"base\")\n",
        "ft_clip    = clipscore_metric(val_records, \"ft\")\n",
        "\n",
        "base_summ  = summac_metric(val_records, \"base\")\n",
        "ft_summ    = summac_metric(val_records, \"ft\")\n",
        "\n",
        "base_fact  = factcc_metric(val_records, \"base\")\n",
        "ft_fact    = factcc_metric(val_records, \"ft\")\n",
        "\n",
        "base_tox   = toxicity_metric(val_records, \"base\")\n",
        "ft_tox     = toxicity_metric(val_records, \"ft\")\n",
        "\n",
        "images_nsfw = nsfw_metric(val_records)\n",
        "\n",
        "base_geval = geval_metric(val_records, \"base\", num_samples=30)\n",
        "ft_geval   = geval_metric(val_records, \"ft\",   num_samples=30)\n",
        "\n",
        "base_all = merge_metrics(base_vqa, base_clip, base_summ, base_fact, base_tox, base_geval)\n",
        "ft_all   = merge_metrics(ft_vqa,   ft_clip,  ft_summ,  ft_fact,  ft_tox,  ft_geval)\n",
        "\n",
        "print(\"\\n=== METRIC COMPARISON (base vs finetuned) on val subset ===\")\n",
        "for k in sorted(base_all.keys()):\n",
        "    print(f\"{k:28s}  base={base_all[k]:.4f}   ft={ft_all[k]:.4f}\")\n",
        "\n",
        "print(\"\\n=== IMAGE SAFETY (dataset images) ===\")\n",
        "for k, v in images_nsfw.items():\n",
        "    print(f\"{k:28s}  {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf541ed-49c3-406b-868f-3ccad5d5b65a",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "957f47c6ca7a4eb4b306722f03278e3d",
            "dfc7f6cb971f419992f8c78ecff2ac8b"
          ]
        },
        "id": "2bf541ed-49c3-406b-868f-3ccad5d5b65a",
        "outputId": "1399758d-cdec-4390-921d-f2a77a85a6e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "⚠️ SummaC not available: No module named 'summac.model'\n",
            "Loading base model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "957f47c6ca7a4eb4b306722f03278e3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading finetuned model + adapter...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfc7f6cb971f419992f8c78ecff2ac8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting outputs on 300 val examples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Collecting outputs: 100%|██████████| 300/300 [06:13<00:00,  1.25s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading FactCC model...\n",
            "Loading Detoxify toxicity model...\n",
            "Loading NSFW image detection pipeline...\n",
            "⚠️ NSFW pipeline not available, skipping NSFW metric: No module named 'transformers.models.ijepa'\n",
            "\n",
            "Computing metrics...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (97 > 77). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 1 column 1 (char 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 425\u001b[39m\n\u001b[32m    421\u001b[39m ft_tox     = toxicity_metric(val_records, \u001b[33m\"\u001b[39m\u001b[33mft\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    423\u001b[39m images_nsfw = nsfw_metric(val_records) \u001b[38;5;28;01mif\u001b[39;00m HAS_NSFW \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m base_geval = \u001b[43mgeval_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbase\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m ft_geval   = geval_metric(val_records, \u001b[33m\"\u001b[39m\u001b[33mft\u001b[39m\u001b[33m\"\u001b[39m,   num_samples=\u001b[32m20\u001b[39m)\n\u001b[32m    428\u001b[39m base_all = merge_metrics(base_vqa, base_clip, base_summ, base_fact, base_tox, base_geval)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 395\u001b[39m, in \u001b[36mgeval_metric\u001b[39m\u001b[34m(records, model_prefix, num_samples)\u001b[39m\n\u001b[32m    393\u001b[39m fl, rel, corr = [], [], []\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m records[:num_samples]:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     f, r_, c = \u001b[43mgeval_score_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m     fl.append(f); rel.append(r_); corr.append(c)\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    398\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mG_eval_fluency\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np.mean(fl)),\n\u001b[32m    399\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mG_eval_relevance\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np.mean(rel)),\n\u001b[32m    400\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mG_eval_correctness\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np.mean(corr)),\n\u001b[32m    401\u001b[39m }\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 387\u001b[39m, in \u001b[36mgeval_score_one\u001b[39m\u001b[34m(record, model_prefix)\u001b[39m\n\u001b[32m    379\u001b[39m resp = client.responses.create(\n\u001b[32m    380\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    381\u001b[39m     \u001b[38;5;28minput\u001b[39m=[\n\u001b[32m   (...)\u001b[39m\u001b[32m    384\u001b[39m     ],\n\u001b[32m    385\u001b[39m )\n\u001b[32m    386\u001b[39m text = resp.output[\u001b[32m0\u001b[39m].content[\u001b[32m0\u001b[39m].text\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m scores = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scores[\u001b[33m\"\u001b[39m\u001b[33mfluency\u001b[39m\u001b[33m\"\u001b[39m], scores[\u001b[33m\"\u001b[39m\u001b[33mrelevance\u001b[39m\u001b[33m\"\u001b[39m], scores[\u001b[33m\"\u001b[39m\u001b[33mcorrectness\u001b[39m\u001b[33m\"\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    333\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     end = _w(s, end).end()\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    353\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
            "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ],
      "source": [
        "# ======================================\n",
        "# ONE-CELL EVAL: BASE VS FINETUNED QWEN2-VL\n",
        "# ======================================\n",
        "# If needed, install once in a separate cell:\n",
        "# !pip install torchmetrics summac detoxify openai torchvision\n",
        "\n",
        "import os, json, re\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import (\n",
        "    Qwen2VLForConditionalGeneration,\n",
        "    Qwen2VLProcessor,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    pipeline,\n",
        ")\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------------\n",
        "# Soft imports / feature flags\n",
        "# -----------------------------\n",
        "# CLIPScore + torchvision\n",
        "try:\n",
        "    from torchmetrics.multimodal import CLIPScore\n",
        "    from torchvision import transforms\n",
        "    HAS_CLIP = True\n",
        "except Exception as e:\n",
        "    print(\"⚠️ CLIPScore not available:\", e)\n",
        "    HAS_CLIP = False\n",
        "\n",
        "# SummaC\n",
        "try:\n",
        "    from summac.model import SummaCZS\n",
        "    HAS_SUMMAC = True\n",
        "except Exception as e:\n",
        "    print(\"⚠️ SummaC not available:\", e)\n",
        "    HAS_SUMMAC = False\n",
        "\n",
        "# Detoxify\n",
        "try:\n",
        "    from detoxify import Detoxify\n",
        "    HAS_DETOXIFY = True\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Detoxify not available:\", e)\n",
        "    HAS_DETOXIFY = False\n",
        "\n",
        "# OpenAI client (from OPENAI_API_KEY variable in notebook, not env)\n",
        "def maybe_load_openai_client_from_var():\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "    except Exception:\n",
        "        print(\"⚠️ openai package not available: G-Eval will be skipped.\")\n",
        "        return None\n",
        "    key = globals().get(\"OPENAI_API_KEY\", None)\n",
        "    if not key:\n",
        "        print(\"⚠️ OPENAI_API_KEY variable not set: G-Eval will be skipped.\")\n",
        "        return None\n",
        "    return OpenAI(api_key=key)\n",
        "\n",
        "client = maybe_load_openai_client_from_var()\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset helpers (match training)\n",
        "# -----------------------------\n",
        "system_message = \"\"\"You are a helpful science tutor.\n",
        "You see a question, several answer choices, and often an image and a hint/lecture.\n",
        "Your job is to pick the single correct multiple-choice option.\n",
        "Start your answer with ONLY the letter of the correct option (A, B, C, D, etc.),\n",
        "then a period, then a short explanation.\n",
        "Example: \"C. Because ...\"\n",
        "\"\"\"\n",
        "\n",
        "LETTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "\n",
        "def index_to_letter(idx: int) -> str:\n",
        "    return LETTERS[idx]\n",
        "\n",
        "def build_question_text(example) -> str:\n",
        "    choices = example[\"choices\"]\n",
        "    options_text = \"\\n\".join(\n",
        "        f\"({LETTERS[i]}) {choice}\" for i, choice in enumerate(choices)\n",
        "    )\n",
        "    hint = example.get(\"hint\", \"\")\n",
        "    lecture = example.get(\"lecture\", \"\")\n",
        "    hint_part = f\"\\nHint: {hint}\" if hint else \"\"\n",
        "    lecture_part = f\"\\nLecture: {lecture}\" if lecture else \"\"\n",
        "    return (\n",
        "        f\"Question: {example['question']}\\n\\n\"\n",
        "        f\"Choices:\\n{options_text}\"\n",
        "        f\"{hint_part}{lecture_part}\\n\\n\"\n",
        "        f\"Respond starting with the letter, then a period, then a brief explanation.\"\n",
        "    )\n",
        "\n",
        "def format_example(example):\n",
        "    correct_letter = index_to_letter(int(example[\"answer\"]))\n",
        "    explanation = example.get(\"solution\", \"\") or example.get(\"explanation\", \"\")\n",
        "    if explanation:\n",
        "        assistant_text = f\"{correct_letter}. {explanation}\"\n",
        "    else:\n",
        "        assistant_text = correct_letter\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": example[\"image\"]},\n",
        "                {\"type\": \"text\", \"text\": build_question_text(example)},\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": assistant_text}],\n",
        "        },\n",
        "    ]\n",
        "    return {\"images\": [example[\"image\"]], \"messages\": messages}\n",
        "\n",
        "def parse_letter_and_explanation(text: str):\n",
        "    m = re.match(r\"\\s*([A-Z])\\s*[\\.\\-\\):]\\s*(.*)\", text, flags=re.DOTALL)\n",
        "    if m:\n",
        "        letter = m.group(1)\n",
        "        explanation = m.group(2).strip()\n",
        "    else:\n",
        "        m2 = re.search(r\"[A-Z]\", text)\n",
        "        if m2:\n",
        "            letter = m2.group(0)\n",
        "            explanation = text[m2.end():].strip()\n",
        "        else:\n",
        "            letter = None\n",
        "            explanation = text.strip()\n",
        "    return letter, explanation\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Load base & finetuned\n",
        "# -----------------------------\n",
        "BASE_MODEL_ID = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "FT_ADAPTER_DIR = \"qwen2-vl-2b-scienceqa-lora-expl\"  # change if needed\n",
        "\n",
        "print(\"Loading base model...\")\n",
        "base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "base_model.eval()\n",
        "base_processor = Qwen2VLProcessor.from_pretrained(BASE_MODEL_ID)\n",
        "\n",
        "print(\"Loading finetuned model + adapter...\")\n",
        "ft_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "ft_model.load_adapter(FT_ADAPTER_DIR)\n",
        "ft_model.eval()\n",
        "ft_processor = Qwen2VLProcessor.from_pretrained(BASE_MODEL_ID)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Shared generation helper\n",
        "# -----------------------------\n",
        "def generate_answer_with_explanation_generic(model, processor, example, max_new_tokens=96):\n",
        "    formatted = format_example(example)\n",
        "    conv_for_gen = formatted[\"messages\"][:2]  # system + user\n",
        "\n",
        "    text_prompt = processor.apply_chat_template(\n",
        "        conv_for_gen,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    image_inputs, _ = process_vision_info(formatted[\"messages\"])\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text_prompt],\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gen_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "\n",
        "    trimmed = [\n",
        "        out_ids[len(in_ids):]\n",
        "        for in_ids, out_ids in zip(inputs.input_ids, gen_ids)\n",
        "    ]\n",
        "\n",
        "    out_text = processor.batch_decode(\n",
        "        trimmed,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False,\n",
        "    )[0].strip()\n",
        "\n",
        "    return out_text\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Collect outputs\n",
        "# -----------------------------\n",
        "def collect_outputs(dataset, num_samples=None, max_new_tokens=96):\n",
        "    n = len(dataset) if num_samples is None else min(num_samples, len(dataset))\n",
        "    records = []\n",
        "    for i in tqdm(range(n), desc=\"Collecting outputs\"):\n",
        "        ex = dataset[i]\n",
        "        true_letter = index_to_letter(int(ex[\"answer\"]))\n",
        "        solution = ex.get(\"solution\", \"\")\n",
        "\n",
        "        base_raw = generate_answer_with_explanation_generic(\n",
        "            base_model, base_processor, ex, max_new_tokens=max_new_tokens\n",
        "        )\n",
        "        base_letter, base_expl = parse_letter_and_explanation(base_raw)\n",
        "\n",
        "        ft_raw = generate_answer_with_explanation_generic(\n",
        "            ft_model, ft_processor, ex, max_new_tokens=max_new_tokens\n",
        "        )\n",
        "        ft_letter, ft_expl = parse_letter_and_explanation(ft_raw)\n",
        "\n",
        "        source_text = build_question_text(ex) + \"\\n\\n\" + (solution or \"\")\n",
        "\n",
        "        records.append(\n",
        "            {\n",
        "                \"example\": ex,\n",
        "                \"true_letter\": true_letter,\n",
        "                \"solution\": solution,\n",
        "                \"source_text\": source_text,\n",
        "                \"base_raw\": base_raw,\n",
        "                \"base_letter\": base_letter,\n",
        "                \"base_expl\": base_expl,\n",
        "                \"ft_raw\": ft_raw,\n",
        "                \"ft_letter\": ft_letter,\n",
        "                \"ft_expl\": ft_expl,\n",
        "            }\n",
        "        )\n",
        "    return records\n",
        "\n",
        "NUM_VAL_EXAMPLES = 300  # change if you want more/less\n",
        "print(f\"Collecting outputs on {NUM_VAL_EXAMPLES} val examples...\")\n",
        "val_records = collect_outputs(val_ds, num_samples=NUM_VAL_EXAMPLES, max_new_tokens=96)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6. SummaC & FactCC\n",
        "# -----------------------------\n",
        "if HAS_SUMMAC:\n",
        "    print(\"Loading SummaC model...\")\n",
        "    summac_model = SummaCZS(granularity=\"sentence\", model_name=\"vitc\")\n",
        "\n",
        "    def summac_metric(records, model_prefix=\"base\"):\n",
        "        sources = [r[\"source_text\"] for r in records]\n",
        "        summaries = [r[f\"{model_prefix}_expl\"] or \"\" for r in records]\n",
        "        scores = summac_model.score(sources=sources, summaries=summaries)[\"scores\"]\n",
        "        return {\"SummaC_mean\": float(np.mean(scores))}\n",
        "else:\n",
        "    def summac_metric(records, model_prefix=\"base\"):\n",
        "        return {}\n",
        "\n",
        "print(\"Loading FactCC model...\")\n",
        "factcc_model_id = \"manueldeprada/FactCC\"\n",
        "factcc_tokenizer = AutoTokenizer.from_pretrained(factcc_model_id)\n",
        "factcc_model = AutoModelForSequenceClassification.from_pretrained(factcc_model_id).to(device)\n",
        "factcc_model.eval()\n",
        "\n",
        "def factcc_metric(records, model_prefix=\"base\", batch_size=16):\n",
        "    probs_all = []\n",
        "    for i in range(0, len(records), batch_size):\n",
        "        batch = records[i:i+batch_size]\n",
        "        sources = [r[\"source_text\"] for r in batch]\n",
        "        summaries = [r[f\"{model_prefix}_expl\"] or \"\" for r in batch]\n",
        "        enc = factcc_tokenizer(\n",
        "            sources,\n",
        "            summaries,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = factcc_model(**enc).logits\n",
        "            probs = torch.softmax(logits, dim=-1)[:, 1]  # assume label 1 = factual\n",
        "        probs_all.extend(probs.cpu().tolist())\n",
        "    return {\"FactCC_mean_prob_factual\": float(np.mean(probs_all))}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 9. Compute all metrics\n",
        "# -----------------------------\n",
        "def merge_metrics(*metric_dicts):\n",
        "    merged = {}\n",
        "    for d in metric_dicts:\n",
        "        merged.update(d)\n",
        "    return merged\n",
        "\n",
        "print(\"\\nComputing metrics...\")\n",
        "\n",
        "base_vqa   = vqa_metrics(val_records, \"base\")\n",
        "ft_vqa     = vqa_metrics(val_records, \"ft\")\n",
        "\n",
        "base_clip  = clipscore_metric(val_records, \"base\")\n",
        "ft_clip    = clipscore_metric(val_records, \"ft\")\n",
        "\n",
        "base_summ  = summac_metric(val_records, \"base\")\n",
        "ft_summ    = summac_metric(val_records, \"ft\")\n",
        "\n",
        "base_fact  = factcc_metric(val_records, \"base\")\n",
        "ft_fact    = factcc_metric(val_records, \"ft\")\n",
        "\n",
        "base_tox   = toxicity_metric(val_records, \"base\")\n",
        "ft_tox     = toxicity_metric(val_records, \"ft\")\n",
        "\n",
        "images_nsfw = nsfw_metric(val_records) if HAS_NSFW else {}\n",
        "\n",
        "base_geval = geval_metric(val_records, \"base\", num_samples=20)\n",
        "ft_geval   = geval_metric(val_records, \"ft\",   num_samples=20)\n",
        "\n",
        "base_all = merge_metrics(base_vqa, base_clip, base_summ, base_fact, base_tox, base_geval)\n",
        "ft_all   = merge_metrics(ft_vqa,   ft_clip,  ft_summ,  ft_fact,  ft_tox,  ft_geval)\n",
        "\n",
        "print(\"\\n=== METRIC COMPARISON (base vs finetuned) on val subset ===\")\n",
        "for k in sorted(base_all.keys()):\n",
        "    print(f\"{k:28s}  base={base_all[k]:.4f}   ft={ft_all[k]:.4f}\")\n",
        "\n",
        "if HAS_NSFW:\n",
        "    print(\"\\n=== IMAGE SAFETY (NSFW) ===\")\n",
        "    for k, v in images_nsfw.items():\n",
        "        print(f\"{k:28s}  {v:.4f}\")\n",
        "else:\n",
        "    print(\"\\n=== IMAGE SAFETY (NSFW) ===\")\n",
        "    print(\"NSFW metric skipped (pipeline not available).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9f55799-9014-486b-92db-13441d7802a7",
      "metadata": {
        "id": "b9f55799-9014-486b-92db-13441d7802a7"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 4. VQA metrics (EM / F1)\n",
        "# -----------------------------\n",
        "def _token_f1(pred, gold):\n",
        "    pred_tokens = pred.lower().split()\n",
        "    gold_tokens = gold.lower().split()\n",
        "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "        return 0.0\n",
        "    common = Counter(pred_tokens) & Counter(gold_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0.0\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(gold_tokens)\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "def vqa_metrics(records, model_prefix=\"base\"):\n",
        "    em_list, f1_list = [], []\n",
        "    for r in records:\n",
        "        true_letter = r[\"true_letter\"]\n",
        "        sol = r[\"solution\"] or \"\"\n",
        "        pred_letter = r[f\"{model_prefix}_letter\"]\n",
        "        pred_expl = r[f\"{model_prefix}_expl\"] or \"\"\n",
        "        em_list.append(1.0 if pred_letter == true_letter else 0.0)\n",
        "        if sol:\n",
        "            f1_list.append(_token_f1(pred_expl, sol))\n",
        "    em = float(np.mean(em_list))\n",
        "    f1 = float(np.mean(f1_list)) if f1_list else 0.0\n",
        "    return {\"VQA_EM\": em, \"VQA_F1_expl\": f1}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a59fce7-2dde-4f9e-95f8-35709d7cc663",
      "metadata": {
        "id": "5a59fce7-2dde-4f9e-95f8-35709d7cc663"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 5. CLIPScore (if available)\n",
        "# -----------------------------\n",
        "if HAS_CLIP:\n",
        "    clip_metric = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\").to(device)\n",
        "    to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def clipscore_metric(records, model_prefix=\"base\"):\n",
        "        images, texts = [], []\n",
        "        for r in records:\n",
        "            img_pil = r[\"example\"][\"image\"]\n",
        "            text = r[f\"{model_prefix}_raw\"]\n",
        "            img_tensor = to_tensor(img_pil)\n",
        "            images.append(img_tensor)\n",
        "            texts.append(text)\n",
        "        with torch.no_grad():\n",
        "            score = clip_metric(images, texts)\n",
        "        return {\"CLIPScore\": float(score.item())}\n",
        "else:\n",
        "    def clipscore_metric(records, model_prefix=\"base\"):\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb5a52d4-553c-4fc5-a6ce-849d9343a67b",
      "metadata": {
        "id": "eb5a52d4-553c-4fc5-a6ce-849d9343a67b"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 7. G-Eval (LLM-as-judge, optional)\n",
        "# -----------------------------\n",
        "GEVAL_SYSTEM_PROMPT = \"\"\"\n",
        "You are grading answers to science visual question answering problems.\n",
        "\n",
        "You will receive:\n",
        "- the question and answer choices\n",
        "- the reference correct answer letter\n",
        "- the model's predicted answer and explanation\n",
        "\n",
        "For each output, give three scores from 1 to 5:\n",
        "- Fluency: Is the explanation clear, grammatical and well-written?\n",
        "- Relevance: Does the explanation focus on the question and image/context?\n",
        "- Correctness: Is the predicted answer letter correct and is the explanation logically consistent with the context?\n",
        "\n",
        "Respond ONLY in JSON with keys: fluency, relevance, correctness.\n",
        "\"\"\"\n",
        "\n",
        "def geval_score_one(record, model_prefix=\"base\"):\n",
        "    ex = record[\"example\"]\n",
        "    q_block = build_question_text(ex)\n",
        "    ref_letter = record[\"true_letter\"]\n",
        "    pred_raw = record[f\"{model_prefix}_raw\"]\n",
        "\n",
        "    user_content = f\"\"\"\n",
        "QUESTION & CONTEXT:\n",
        "{q_block}\n",
        "\n",
        "REFERENCE ANSWER LETTER: {ref_letter}\n",
        "\n",
        "MODEL OUTPUT:\n",
        "{pred_raw}\n",
        "\"\"\"\n",
        "    resp = client.responses.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        input=[\n",
        "            {\"role\": \"system\", \"content\": GEVAL_SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "        ],\n",
        "    )\n",
        "    text = resp.output[0].content[0].text\n",
        "    scores = json.loads(text)\n",
        "    return scores[\"fluency\"], scores[\"relevance\"], scores[\"correctness\"]\n",
        "\n",
        "def geval_metric(records, model_prefix=\"base\", num_samples=30):\n",
        "    if client is None:\n",
        "        return {}\n",
        "    fl, rel, corr = [], [], []\n",
        "    for r in records[:num_samples]:\n",
        "        f, r_, c = geval_score_one(r, model_prefix)\n",
        "        fl.append(f); rel.append(r_); corr.append(c)\n",
        "    return {\n",
        "        \"G_eval_fluency\": float(np.mean(fl)),\n",
        "        \"G_eval_relevance\": float(np.mean(rel)),\n",
        "        \"G_eval_correctness\": float(np.mean(corr)),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bcb8dce-52fc-40fe-9e69-f45dde281f19",
      "metadata": {
        "id": "4bcb8dce-52fc-40fe-9e69-f45dde281f19",
        "outputId": "6bbba73a-7301-4007-9900-e16cf830c64a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Detoxify toxicity model...\n",
            "Loading NSFW image detection pipeline...\n",
            "⚠️ NSFW pipeline not available, skipping NSFW metric: No module named 'transformers.models.ijepa'\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# 8. Safety: toxicity + NSFW\n",
        "# -----------------------------\n",
        "if HAS_DETOXIFY:\n",
        "    print(\"Loading Detoxify toxicity model...\")\n",
        "    tox_model = Detoxify(\"unbiased\")\n",
        "\n",
        "    def toxicity_metric(records, model_prefix=\"base\"):\n",
        "        texts = [r[f\"{model_prefix}_raw\"] for r in records]\n",
        "        scores = tox_model.predict(texts)[\"toxicity\"]\n",
        "        return {\n",
        "            \"toxicity_mean\": float(np.mean(scores)),\n",
        "            \"toxicity_95p\": float(np.percentile(scores, 95)),\n",
        "        }\n",
        "else:\n",
        "    def toxicity_metric(records, model_prefix=\"base\"):\n",
        "        return {}\n",
        "\n",
        "# NSFW image detection: wrap in try/except so it NEVER crashes\n",
        "HAS_NSFW = True\n",
        "try:\n",
        "    print(\"Loading NSFW image detection pipeline...\")\n",
        "    nsfw_pipe = pipeline(\n",
        "        \"image-classification\",\n",
        "        model=\"Falconsai/nsfw_image_detection\",\n",
        "        device=0 if device.type == \"cuda\" else -1,\n",
        "    )\n",
        "\n",
        "    def nsfw_metric(records):\n",
        "        nsfw_probs = []\n",
        "        for r in records:\n",
        "            img = r[\"example\"][\"image\"]\n",
        "            preds = nsfw_pipe(img)\n",
        "            prob_nsfw = 0.0\n",
        "            for p in preds:\n",
        "                if \"nsfw\" in p[\"label\"].lower():\n",
        "                    prob_nsfw = p[\"score\"]\n",
        "                    break\n",
        "            nsfw_probs.append(prob_nsfw)\n",
        "        return {\n",
        "            \"NSFW_mean\": float(np.mean(nsfw_probs)),\n",
        "            \"NSFW_95p\": float(np.percentile(nsfw_probs, 95)),\n",
        "        }\n",
        "except Exception as e:\n",
        "    HAS_NSFW = False\n",
        "    print(\"⚠️ NSFW pipeline not available, skipping NSFW metric:\", e)\n",
        "\n",
        "    def nsfw_metric(records):\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6eaa830-d9b7-4752-bead-73d78780f3d1",
      "metadata": {
        "id": "b6eaa830-d9b7-4752-bead-73d78780f3d1"
      },
      "outputs": [],
      "source": [
        "def geval_metric(records, model_prefix=\"base\", num_samples=30):\n",
        "    return {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb9e9d3c-c84d-452e-b983-b262c69d3226",
      "metadata": {
        "id": "cb9e9d3c-c84d-452e-b983-b262c69d3226",
        "outputId": "d4c32cf4-dd15-4b2c-8153-4ee0e7a0eebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Computing metrics...\n",
            "\n",
            "=== METRIC COMPARISON (base vs finetuned) on val subset ===\n",
            "CLIPScore                     base=23.0492   ft=22.4087\n",
            "FactCC_mean_prob_factual      base=0.4172   ft=0.1877\n",
            "VQA_EM                        base=0.6500   ft=0.6733\n",
            "VQA_F1_expl                   base=0.2056   ft=0.7695\n",
            "toxicity_95p                  base=0.0143   ft=0.0050\n",
            "toxicity_mean                 base=0.0031   ft=0.0018\n",
            "\n",
            "=== IMAGE SAFETY (NSFW) ===\n",
            "NSFW metric skipped (pipeline not available).\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nComputing metrics...\")\n",
        "\n",
        "base_vqa   = vqa_metrics(val_records, \"base\")\n",
        "ft_vqa     = vqa_metrics(val_records, \"ft\")\n",
        "\n",
        "base_clip  = clipscore_metric(val_records, \"base\")\n",
        "ft_clip    = clipscore_metric(val_records, \"ft\")\n",
        "\n",
        "base_summ  = summac_metric(val_records, \"base\")\n",
        "ft_summ    = summac_metric(val_records, \"ft\")\n",
        "\n",
        "base_fact  = factcc_metric(val_records, \"base\")\n",
        "ft_fact    = factcc_metric(val_records, \"ft\")\n",
        "\n",
        "base_tox   = toxicity_metric(val_records, \"base\")\n",
        "ft_tox     = toxicity_metric(val_records, \"ft\")\n",
        "\n",
        "images_nsfw = nsfw_metric(val_records) if HAS_NSFW else {}\n",
        "\n",
        "base_geval = geval_metric(val_records, \"base\", num_samples=20)\n",
        "ft_geval   = geval_metric(val_records, \"ft\",   num_samples=20)\n",
        "\n",
        "base_all = merge_metrics(base_vqa, base_clip, base_summ, base_fact, base_tox, base_geval)\n",
        "ft_all   = merge_metrics(ft_vqa,   ft_clip,  ft_summ,  ft_fact,  ft_tox,  ft_geval)\n",
        "\n",
        "print(\"\\n=== METRIC COMPARISON (base vs finetuned) on val subset ===\")\n",
        "for k in sorted(base_all.keys()):\n",
        "    print(f\"{k:28s}  base={base_all[k]:.4f}   ft={ft_all[k]:.4f}\")\n",
        "\n",
        "if HAS_NSFW:\n",
        "    print(\"\\n=== IMAGE SAFETY (NSFW) ===\")\n",
        "    for k, v in images_nsfw.items():\n",
        "        print(f\"{k:28s}  {v:.4f}\")\n",
        "else:\n",
        "    print(\"\\n=== IMAGE SAFETY (NSFW) ===\")\n",
        "    print(\"NSFW metric skipped (pipeline not available).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b5873db-5bfe-421a-90d7-9bae11766639",
      "metadata": {
        "id": "2b5873db-5bfe-421a-90d7-9bae11766639",
        "outputId": "38e49aea-2127-4cf3-eddc-f7151194ec62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Collecting VQA outputs (img/no-img): 100%|██████████| 300/300 [07:38<00:00,  1.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Overall MC Accuracy (img vs no-image) ===\n",
            "BASE        with image: 0.6500   text-only: 0.5633   Δ=+0.0867\n",
            "FINETUNED   with image: 0.6733   text-only: 0.5567   Δ=+0.1167\n",
            "\n",
            "=== Per-subject accuracy (with image) ===\n",
            "language science  base=0.7143   ft=0.7143\n",
            "natural science  base=0.5722   ft=0.5979\n",
            "social science   base=0.7980   ft=0.8182\n",
            "\n",
            "=== Per-grade accuracy (with image) ===\n",
            "Grade grade1  base=0.0000   ft=0.0000\n",
            "Grade grade2  base=0.6667   ft=0.7143\n",
            "Grade grade3  base=0.6744   ft=0.7442\n",
            "Grade grade4  base=0.7612   ft=0.7015\n",
            "Grade grade5  base=0.7907   ft=0.7674\n",
            "Grade grade6  base=0.4444   ft=0.6000\n",
            "Grade grade7  base=0.6486   ft=0.5946\n",
            "Grade grade8  base=0.5349   ft=0.6047\n",
            "\n",
            "=== Per-true-option accuracy (with image) ===\n",
            "Option A:  base=0.5684   ft=0.6211\n",
            "Option B:  base=0.7109   ft=0.6719\n",
            "Option C:  base=0.5510   ft=0.6327\n",
            "Option D:  base=0.8519   ft=0.9630\n",
            "Option E:  base=0.0000   ft=0.0000\n",
            "\n",
            "=== Prediction distribution (what the model selects) ===\n",
            "BASE with image:\n",
            "{'B': 0.44333333333333336, 'A': 0.29, 'D': 0.10666666666666667, 'C': 0.16}\n",
            "FINETUNED with image:\n",
            "{'B': 0.4, 'A': 0.32, 'D': 0.11333333333333333, 'C': 0.16666666666666666}\n",
            "\n",
            "=== Image reliance (accuracy drop if image removed) ===\n",
            "BASE        Δ (img - no-img) = +0.0867\n",
            "FINETUNED   Δ (img - no-img) = +0.1167\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# VQA-SPECIFIC EVAL SUITE\n",
        "# ============================\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers (reuse from training)\n",
        "# -----------------------------\n",
        "LETTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "\n",
        "def index_to_letter(idx: int) -> str:\n",
        "    return LETTERS[idx]\n",
        "\n",
        "system_message = \"\"\"You are a helpful science tutor.\n",
        "You see a question, several answer choices, and often an image and a hint/lecture.\n",
        "Your job is to pick the single correct multiple-choice option.\n",
        "Start your answer with ONLY the letter of the correct option (A, B, C, D, etc.),\n",
        "then a period, then a short explanation.\n",
        "Example: \"C. Because ...\"\n",
        "\"\"\"\n",
        "\n",
        "def build_question_text(example) -> str:\n",
        "    choices = example[\"choices\"]\n",
        "    options_text = \"\\n\".join(\n",
        "        f\"({LETTERS[i]}) {choice}\" for i, choice in enumerate(choices)\n",
        "    )\n",
        "    hint = example.get(\"hint\", \"\")\n",
        "    lecture = example.get(\"lecture\", \"\")\n",
        "    hint_part = f\"\\nHint: {hint}\" if hint else \"\"\n",
        "    lecture_part = f\"\\nLecture: {lecture}\" if lecture else \"\"\n",
        "    return (\n",
        "        f\"Question: {example['question']}\\n\\n\"\n",
        "        f\"Choices:\\n{options_text}\"\n",
        "        f\"{hint_part}{lecture_part}\\n\\n\"\n",
        "        f\"Respond starting with the letter, then a period, then a brief explanation.\"\n",
        "    )\n",
        "\n",
        "def parse_letter_and_explanation(text: str):\n",
        "    m = re.match(r\"\\s*([A-Z])\\s*[\\.\\-\\):]\\s*(.*)\", text, flags=re.DOTALL)\n",
        "    if m:\n",
        "        letter = m.group(1)\n",
        "        explanation = m.group(2).strip()\n",
        "    else:\n",
        "        m2 = re.search(r\"[A-Z]\", text)\n",
        "        if m2:\n",
        "            letter = m2.group(0)\n",
        "            explanation = text[m2.end():].strip()\n",
        "        else:\n",
        "            letter = None\n",
        "            explanation = text.strip()\n",
        "    return letter, explanation\n",
        "\n",
        "# -----------------------------\n",
        "# Generation with/without image\n",
        "# -----------------------------\n",
        "def generate_answer_mc(\n",
        "    model,\n",
        "    processor,\n",
        "    example,\n",
        "    use_image: bool = True,\n",
        "    max_new_tokens: int = 32,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate \"letter + explanation\" answer.\n",
        "    If use_image=False, we drop the image and do text-only QA.\n",
        "    \"\"\"\n",
        "    # Build messages\n",
        "    user_content = []\n",
        "    if use_image:\n",
        "        user_content.append({\"type\": \"image\", \"image\": example[\"image\"]})\n",
        "    user_content.append({\"type\": \"text\", \"text\": build_question_text(example)})\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]},\n",
        "        {\"role\": \"user\", \"content\": user_content},\n",
        "    ]\n",
        "\n",
        "    # Chat template\n",
        "    text_prompt = processor.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    if use_image:\n",
        "        image_inputs, _ = process_vision_info(messages)\n",
        "    else:\n",
        "        image_inputs = None\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text_prompt],\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gen_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "\n",
        "    trimmed = [\n",
        "        out_ids[len(in_ids):]\n",
        "        for in_ids, out_ids in zip(inputs.input_ids, gen_ids)\n",
        "    ]\n",
        "\n",
        "    out_text = processor.batch_decode(\n",
        "        trimmed,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False,\n",
        "    )[0].strip()\n",
        "\n",
        "    letter, expl = parse_letter_and_explanation(out_text)\n",
        "    return letter, expl, out_text\n",
        "\n",
        "# -----------------------------\n",
        "# Collect VQA outputs\n",
        "# -----------------------------\n",
        "def collect_vqa_records(dataset, num_samples=None, max_new_tokens=32):\n",
        "    \"\"\"\n",
        "    For each example:\n",
        "      - base model with image\n",
        "      - base model without image\n",
        "      - finetuned with image\n",
        "      - finetuned without image\n",
        "    \"\"\"\n",
        "    n = len(dataset) if num_samples is None else min(num_samples, len(dataset))\n",
        "    records = []\n",
        "    for i in tqdm(range(n), desc=\"Collecting VQA outputs (img/no-img)\"):\n",
        "        ex = dataset[i]\n",
        "        true_letter = index_to_letter(int(ex[\"answer\"]))\n",
        "\n",
        "        # Base with image\n",
        "        b_img_letter, b_img_expl, b_img_raw = generate_answer_mc(\n",
        "            base_model, base_processor, ex, use_image=True, max_new_tokens=max_new_tokens\n",
        "        )\n",
        "        # Base without image (text-only)\n",
        "        b_txt_letter, b_txt_expl, b_txt_raw = generate_answer_mc(\n",
        "            base_model, base_processor, ex, use_image=False, max_new_tokens=max_new_tokens\n",
        "        )\n",
        "\n",
        "        # Finetuned with image\n",
        "        ft_img_letter, ft_img_expl, ft_img_raw = generate_answer_mc(\n",
        "            ft_model, ft_processor, ex, use_image=True, max_new_tokens=max_new_tokens\n",
        "        )\n",
        "        # Finetuned without image\n",
        "        ft_txt_letter, ft_txt_expl, ft_txt_raw = generate_answer_mc(\n",
        "            ft_model, ft_processor, ex, use_image=False, max_new_tokens=max_new_tokens\n",
        "        )\n",
        "\n",
        "        records.append(\n",
        "            {\n",
        "                \"example\": ex,\n",
        "                \"true_letter\": true_letter,\n",
        "\n",
        "                \"base_letter_img\": b_img_letter,\n",
        "                \"base_expl_img\": b_img_expl,\n",
        "                \"base_raw_img\": b_img_raw,\n",
        "\n",
        "                \"base_letter_txt\": b_txt_letter,\n",
        "                \"base_expl_txt\": b_txt_expl,\n",
        "                \"base_raw_txt\": b_txt_raw,\n",
        "\n",
        "                \"ft_letter_img\": ft_img_letter,\n",
        "                \"ft_expl_img\": ft_img_expl,\n",
        "                \"ft_raw_img\": ft_img_raw,\n",
        "\n",
        "                \"ft_letter_txt\": ft_txt_letter,\n",
        "                \"ft_expl_txt\": ft_txt_expl,\n",
        "                \"ft_raw_txt\": ft_txt_raw,\n",
        "            }\n",
        "        )\n",
        "    return records\n",
        "\n",
        "# Run collection (small subset for speed)\n",
        "NUM_VAL_EXAMPLES_VQA = 300  # change as you like\n",
        "vqa_records = collect_vqa_records(val_ds, num_samples=NUM_VAL_EXAMPLES_VQA, max_new_tokens=32)\n",
        "\n",
        "# -----------------------------\n",
        "# Metric helpers\n",
        "# -----------------------------\n",
        "def accuracy(records, pred_key):\n",
        "    vals = []\n",
        "    for r in records:\n",
        "        true_l = r[\"true_letter\"]\n",
        "        pred_l = r[pred_key]\n",
        "        vals.append(1.0 if pred_l == true_l else 0.0)\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "def accuracy_by_group(records, pred_key, group_key):\n",
        "    \"\"\"\n",
        "    Accuracy broken down by a field in the ScienceQA example:\n",
        "      - group_key can be 'subject', 'grade', 'task', 'category', etc.\n",
        "    \"\"\"\n",
        "    groups = defaultdict(list)\n",
        "    for r in records:\n",
        "        ex = r[\"example\"]\n",
        "        group_val = ex.get(group_key, None)\n",
        "        if group_val is None:\n",
        "            continue\n",
        "        true_l = r[\"true_letter\"]\n",
        "        pred_l = r[pred_key]\n",
        "        groups[group_val].append(1.0 if pred_l == true_l else 0.0)\n",
        "    return {g: float(np.mean(vs)) for g, vs in groups.items() if vs}\n",
        "\n",
        "def accuracy_by_true_option(records, pred_key):\n",
        "    \"\"\"\n",
        "    Measures how well the model does for each correct option (A/B/C/D...).\n",
        "    This catches bias like \"model rarely chooses option D as correct\".\n",
        "    \"\"\"\n",
        "    groups = defaultdict(list)\n",
        "    for r in records:\n",
        "        true_l = r[\"true_letter\"]\n",
        "        pred_l = r[pred_key]\n",
        "        groups[true_l].append(1.0 if pred_l == true_l else 0.0)\n",
        "    return {opt: float(np.mean(vs)) for opt, vs in groups.items() if vs}\n",
        "\n",
        "def prediction_distribution(records, pred_key):\n",
        "    \"\"\"\n",
        "    How often the model outputs each option overall.\n",
        "    \"\"\"\n",
        "    counts = Counter()\n",
        "    for r in records:\n",
        "        pred_l = r[pred_key]\n",
        "        if pred_l is not None:\n",
        "            counts[pred_l] += 1\n",
        "    total = sum(counts.values())\n",
        "    return {opt: cnt / total for opt, cnt in counts.items()}\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Overall accuracy (img vs no-img)\n",
        "# -----------------------------\n",
        "print(\"\\n=== Overall MC Accuracy (img vs no-image) ===\")\n",
        "for model_prefix, key_img, key_txt in [\n",
        "    (\"BASE\", \"base_letter_img\", \"base_letter_txt\"),\n",
        "    (\"FINETUNED\", \"ft_letter_img\", \"ft_letter_txt\"),\n",
        "]:\n",
        "    acc_img = accuracy(vqa_records, key_img)\n",
        "    acc_txt = accuracy(vqa_records, key_txt)\n",
        "    print(f\"{model_prefix:10s}  with image: {acc_img:.4f}   text-only: {acc_txt:.4f}   Δ={acc_img-acc_txt:+.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Per-subject & per-grade accuracy\n",
        "# -----------------------------\n",
        "print(\"\\n=== Per-subject accuracy (with image) ===\")\n",
        "base_subj = accuracy_by_group(vqa_records, \"base_letter_img\", \"subject\")\n",
        "ft_subj   = accuracy_by_group(vqa_records, \"ft_letter_img\", \"subject\")\n",
        "\n",
        "for subj in sorted(set(base_subj.keys()) | set(ft_subj.keys())):\n",
        "    b = base_subj.get(subj, float(\"nan\"))\n",
        "    f = ft_subj.get(subj, float(\"nan\"))\n",
        "    print(f\"{subj:15s}  base={b:.4f}   ft={f:.4f}\")\n",
        "\n",
        "print(\"\\n=== Per-grade accuracy (with image) ===\")\n",
        "base_grade = accuracy_by_group(vqa_records, \"base_letter_img\", \"grade\")\n",
        "ft_grade   = accuracy_by_group(vqa_records, \"ft_letter_img\", \"grade\")\n",
        "\n",
        "for g in sorted(set(base_grade.keys()) | set(ft_grade.keys())):\n",
        "    b = base_grade.get(g, float(\"nan\"))\n",
        "    f = ft_grade.get(g, float(\"nan\"))\n",
        "    print(f\"Grade {str(g):5s}  base={b:.4f}   ft={f:.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Per-option accuracy + bias\n",
        "# -----------------------------\n",
        "print(\"\\n=== Per-true-option accuracy (with image) ===\")\n",
        "base_opt = accuracy_by_true_option(vqa_records, \"base_letter_img\")\n",
        "ft_opt   = accuracy_by_true_option(vqa_records, \"ft_letter_img\")\n",
        "\n",
        "for opt in sorted(set(base_opt.keys()) | set(ft_opt.keys())):\n",
        "    b = base_opt.get(opt, float(\"nan\"))\n",
        "    f = ft_opt.get(opt, float(\"nan\"))\n",
        "    print(f\"Option {opt}:  base={b:.4f}   ft={f:.4f}\")\n",
        "\n",
        "print(\"\\n=== Prediction distribution (what the model selects) ===\")\n",
        "print(\"BASE with image:\")\n",
        "print(prediction_distribution(vqa_records, \"base_letter_img\"))\n",
        "print(\"FINETUNED with image:\")\n",
        "print(prediction_distribution(vqa_records, \"ft_letter_img\"))\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Image reliance metric\n",
        "# -----------------------------\n",
        "print(\"\\n=== Image reliance (accuracy drop if image removed) ===\")\n",
        "for model_prefix, key_img, key_txt in [\n",
        "    (\"BASE\", \"base_letter_img\", \"base_letter_txt\"),\n",
        "    (\"FINETUNED\", \"ft_letter_img\", \"ft_letter_txt\"),\n",
        "]:\n",
        "    acc_img = accuracy(vqa_records, key_img)\n",
        "    acc_txt = accuracy(vqa_records, key_txt)\n",
        "    print(f\"{model_prefix:10s}  Δ (img - no-img) = {acc_img - acc_txt:+.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f020e59-7667-4e64-876b-16039784f546",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f24ecb4b73c547aebc7fa3b64cb3ec62"
          ]
        },
        "id": "1f020e59-7667-4e64-876b-16039784f546",
        "outputId": "8eec428a-84f6-4e96-8b5a-3e0a74c72dd4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f24ecb4b73c547aebc7fa3b64cb3ec62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ScienceQA: 100%|██████████| 2097/2097 [02:15<00:00, 15.47it/s]\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'mmbench_dev_en_20231003.tsv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 200\u001b[39m\n\u001b[32m    197\u001b[39m PATH_CHARTQA = \u001b[33m\"\u001b[39m\u001b[33mChartQA/test.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m results[\u001b[33m\"\u001b[39m\u001b[33mScienceQA\u001b[39m\u001b[33m\"\u001b[39m] = evaluate_scienceqa(val_ds)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m results[\u001b[33m\"\u001b[39m\u001b[33mMMBench\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mevaluate_mmbench\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_MMBENCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m results[\u001b[33m\"\u001b[39m\u001b[33mSEED-Bench\u001b[39m\u001b[33m\"\u001b[39m] = evaluate_seed(PATH_SEED)\n\u001b[32m    202\u001b[39m results[\u001b[33m\"\u001b[39m\u001b[33mMMMU-Lite\u001b[39m\u001b[33m\"\u001b[39m] = evaluate_mmmu(PATH_MMMU)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mevaluate_mmbench\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_mmbench\u001b[39m(path):\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     correct = \u001b[32m0\u001b[39m\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df.iterrows(), total=\u001b[38;5;28mlen\u001b[39m(df), desc=\u001b[33m\"\u001b[39m\u001b[33mMMBench\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'mmbench_dev_en_20231003.tsv'"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# UNIFIED MULTI-BENCHMARK EVALUATION SCRIPT\n",
        "# Qwen2-VL-2B (Finetuned) VQA Benchmark Suite\n",
        "# ============================================\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "# ================================\n",
        "# Load finetuned model + processor\n",
        "# ================================\n",
        "FT_ADAPTER_DIR = \"qwen2-vl-2b-scienceqa-lora-expl\"\n",
        "\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "model.load_adapter(FT_ADAPTER_DIR)\n",
        "model.eval()\n",
        "\n",
        "processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
        "\n",
        "# ================================\n",
        "# Universal VQA inference wrapper\n",
        "# ================================\n",
        "def vlm_answer(image, question, max_new_tokens=64):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": question}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    text_prompt = processor.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    img_inputs, _ = process_vision_info(messages)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text_prompt],\n",
        "        images=img_inputs,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    decoded = processor.batch_decode(\n",
        "        out[:, inputs.input_ids.shape[1]:],\n",
        "        skip_special_tokens=True,\n",
        "    )[0].strip()\n",
        "    return decoded\n",
        "\n",
        "# ================================================\n",
        "# 1) SCIENCEQA OFFICIAL ACCURACY (val or test set)\n",
        "# ================================================\n",
        "lettermap = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "\n",
        "def evaluate_scienceqa(dataset):\n",
        "    correct = 0\n",
        "    for ex in tqdm(dataset, desc=\"ScienceQA\"):\n",
        "        img = ex[\"image\"]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"]\n",
        "        gold = lettermap[int(ex[\"answer\"])]\n",
        "\n",
        "        q_full = q + \"\\n\" + \"\\n\".join(\n",
        "            f\"({lettermap[i]}) {c}\" for i, c in enumerate(choices)\n",
        "        ) + \"\\nAnswer with the letter only.\"\n",
        "\n",
        "        pred = vlm_answer(img, q_full)\n",
        "        pred_letter = next((L for L in lettermap if L in pred), None)\n",
        "\n",
        "        if pred_letter == gold:\n",
        "            correct += 1\n",
        "    return correct / len(dataset)\n",
        "\n",
        "# ===============================================\n",
        "# 2) MMBench Accuracy\n",
        "# ===============================================\n",
        "def evaluate_mmbench(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\")\n",
        "    correct = 0\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"MMBench\"):\n",
        "        img = Image.open(row[\"img_path\"]).convert(\"RGB\")\n",
        "        choices = [row[f\"choice_{i}\"] for i in range(1, 9) if pd.notna(row.get(f\"choice_{i}\"))]\n",
        "        ans = row[\"answer\"]\n",
        "\n",
        "        q_full = row[\"question\"] + \"\\n\" + \"\\n\".join(\n",
        "            f\"({lettermap[i]}) {choices[i]}\" for i in range(len(choices))\n",
        "        ) + \"\\nAnswer with the letter only.\"\n",
        "\n",
        "        pred = vlm_answer(img, q_full)\n",
        "        pred_letter = next((L for L in lettermap if L in pred), None)\n",
        "\n",
        "        if pred_letter == ans:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(df)\n",
        "\n",
        "# ===============================================\n",
        "# 3) SEED-Bench (image subset)\n",
        "# ===============================================\n",
        "def evaluate_seed(path):\n",
        "    with open(path) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    correct = 0\n",
        "    for item in tqdm(data, desc=\"SEED\"):\n",
        "        img = Image.open(item[\"image\"]).convert(\"RGB\")\n",
        "        q = item[\"question\"]\n",
        "        choices = item[\"choices\"]\n",
        "        gold = item[\"answer\"]  # A/B/C/D\n",
        "\n",
        "        q_full = q + \"\\n\" + \"\\n\".join(\n",
        "            f\"({lettermap[i]}) {choices[i]}\" for i in range(len(choices))\n",
        "        ) + \"\\nAnswer with the letter only.\"\n",
        "\n",
        "        pred = vlm_answer(img, q_full)\n",
        "        pred_letter = next((L for L in \"ABCD\" if L in pred), None)\n",
        "\n",
        "        if pred_letter == gold:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(data)\n",
        "\n",
        "# ===============================================\n",
        "# 4) MMMU-Lite\n",
        "# ===============================================\n",
        "def evaluate_mmmu(path):\n",
        "    with open(path) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    correct = 0\n",
        "    for ex in tqdm(data, desc=\"MMMU-Lite\"):\n",
        "        img = Image.open(ex[\"image\"]).convert(\"RGB\")\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"options\"]\n",
        "        gold = ex[\"answer\"]\n",
        "\n",
        "        q_full = q + \"\\n\" + \"\\n\".join(\n",
        "            f\"({lettermap[i]}) {choices[i]}\" for i in range(len(choices))\n",
        "        ) + \"\\nAnswer with the letter only.\"\n",
        "\n",
        "        pred = vlm_answer(img, q_full)\n",
        "        pred_letter = next((L for L in lettermap if L in pred), None)\n",
        "\n",
        "        if pred_letter == gold:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(data)\n",
        "\n",
        "# ===============================================\n",
        "# 5) ChartQA (free-form answer)\n",
        "# ===============================================\n",
        "def evaluate_chartqa(path):\n",
        "    with open(path) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    correct = 0\n",
        "    for ex in tqdm(data, desc=\"ChartQA\"):\n",
        "        img = Image.open(ex[\"img_path\"]).convert(\"RGB\")\n",
        "        q = ex[\"query\"]\n",
        "        gold = ex[\"label\"].lower().strip()\n",
        "\n",
        "        pred = vlm_answer(img, q).lower().strip()\n",
        "\n",
        "        if pred == gold:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(data)\n",
        "\n",
        "# ===============================================\n",
        "# RUN ALL BENCHMARKS\n",
        "# ===============================================\n",
        "\n",
        "results = {}\n",
        "\n",
        "# You MUST set paths here:\n",
        "PATH_MMBENCH = \"PATH_MMBENCH = \"https://raw.githubusercontent.com/open-compass/opencompass/main/mmbench/mmbench_dev_en_20231003.tsv\"\n",
        "PATH_SEED = \"SEED-Bench-Image.json\"\n",
        "PATH_MMMU = \"MMMU_lite.json\"\n",
        "PATH_CHARTQA = \"ChartQA/test.json\"\n",
        "\n",
        "results[\"ScienceQA\"] = evaluate_scienceqa(val_ds)\n",
        "results[\"MMBench\"] = evaluate_mmbench(PATH_MMBENCH)\n",
        "results[\"SEED-Bench\"] = evaluate_seed(PATH_SEED)\n",
        "results[\"MMMU-Lite\"] = evaluate_mmmu(PATH_MMMU)\n",
        "results[\"ChartQA\"] = evaluate_chartqa(PATH_CHARTQA)\n",
        "\n",
        "print(\"\\n=== UNIFIED RESULTS TABLE ===\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k:15s} = {v:.4f}\")\n",
        "\n",
        "# ===============================================\n",
        "# RADAR PLOT\n",
        "# ===============================================\n",
        "\n",
        "labels = list(results.keys())\n",
        "scores = [results[k] for k in labels]\n",
        "\n",
        "angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()\n",
        "scores += scores[:1]\n",
        "angles += angles[:1]\n",
        "\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "ax = plt.subplot(111, polar=True)\n",
        "\n",
        "ax.plot(angles, scores, linewidth=2)\n",
        "ax.fill(angles, scores, alpha=0.25)\n",
        "\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_title(\"Qwen2-VL-2B Finetuned — VQA Radar Plot\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1c93833-f720-4e00-a338-1426b9353b74",
      "metadata": {
        "id": "c1c93833-f720-4e00-a338-1426b9353b74",
        "outputId": "e7a3cec1-f213-43f0-c60e-bd0153a90871"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ScienceQA: 100%|██████████| 2097/2097 [02:14<00:00, 15.58it/s]\n"
          ]
        },
        {
          "ename": "HTTPError",
          "evalue": "HTTP Error 404: Not Found",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m PATH_CHARTQA = \u001b[33m\"\u001b[39m\u001b[33mChartQA/test.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m results[\u001b[33m\"\u001b[39m\u001b[33mScienceQA\u001b[39m\u001b[33m\"\u001b[39m] = evaluate_scienceqa(val_ds)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m results[\u001b[33m\"\u001b[39m\u001b[33mMMBench\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mevaluate_mmbench\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_MMBENCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m results[\u001b[33m\"\u001b[39m\u001b[33mSEED-Bench\u001b[39m\u001b[33m\"\u001b[39m] = evaluate_seed(PATH_SEED)\n\u001b[32m     16\u001b[39m results[\u001b[33m\"\u001b[39m\u001b[33mMMMU-Lite\u001b[39m\u001b[33m\"\u001b[39m] = evaluate_mmmu(PATH_MMMU)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mevaluate_mmbench\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_mmbench\u001b[39m(path):\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     correct = \u001b[32m0\u001b[39m\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df.iterrows(), total=\u001b[38;5;28mlen\u001b[39m(df), desc=\u001b[33m\"\u001b[39m\u001b[33mMMBench\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py:728\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    725\u001b[39m     codecs.lookup_error(errors)\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ioargs = \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m handle = ioargs.filepath_or_buffer\n\u001b[32m    737\u001b[39m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py:384\u001b[39m, in \u001b[36m_get_filepath_or_buffer\u001b[39m\u001b[34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[32m    383\u001b[39m req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[32m    385\u001b[39m     content_encoding = req.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Encoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py:289\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[33;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03mthe stdlib.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrequest\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:215\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:521\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    520\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:630\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    628\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:559\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    558\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    491\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:639\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
            "\u001b[31mHTTPError\u001b[39m: HTTP Error 404: Not Found"
          ]
        }
      ],
      "source": [
        "# ===============================================\n",
        "# RUN ALL BENCHMARKS\n",
        "# ===============================================\n",
        "\n",
        "results = {}\n",
        "\n",
        "# You MUST set paths here:\n",
        "PATH_MMBENCH = \"https://raw.githubusercontent.com/open-compass/opencompass/main/mmbench/mmbench_dev_en_20231003.tsv\"\n",
        "PATH_SEED = \"SEED-Bench-Image.json\"\n",
        "PATH_MMMU = \"MMMU_lite.json\"\n",
        "PATH_CHARTQA = \"ChartQA/test.json\"\n",
        "\n",
        "results[\"ScienceQA\"] = evaluate_scienceqa(val_ds)\n",
        "results[\"MMBench\"] = evaluate_mmbench(PATH_MMBENCH)\n",
        "results[\"SEED-Bench\"] = evaluate_seed(PATH_SEED)\n",
        "results[\"MMMU-Lite\"] = evaluate_mmmu(PATH_MMMU)\n",
        "results[\"ChartQA\"] = evaluate_chartqa(PATH_CHARTQA)\n",
        "\n",
        "print(\"\\n=== UNIFIED RESULTS TABLE ===\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k:15s} = {v:.4f}\")\n",
        "\n",
        "# ===============================================\n",
        "# RADAR PLOT\n",
        "# ===============================================\n",
        "\n",
        "labels = list(results.keys())\n",
        "scores = [results[k] for k in labels]\n",
        "\n",
        "angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()\n",
        "scores += scores[:1]\n",
        "angles += angles[:1]\n",
        "\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "ax = plt.subplot(111, polar=True)\n",
        "\n",
        "ax.plot(angles, scores, linewidth=2)\n",
        "ax.fill(angles, scores, alpha=0.25)\n",
        "\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_title(\"Qwen2-VL-2B Finetuned — VQA Radar Plot\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd9e500d-feb5-4c77-8ef8-bf5c21768091",
      "metadata": {
        "id": "dd9e500d-feb5-4c77-8ef8-bf5c21768091",
        "outputId": "77d45820-85a3-483b-e0e8-f5e12c9916b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.61.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (113 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=3 in /usr/lib/python3/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m147.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.61.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m185.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [matplotlib]5\u001b[0m [matplotlib]\n",
            "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.0 kiwisolver-1.4.9 matplotlib-3.10.7\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6db8fa2-3697-4aee-bd7a-9bdc4cab4015",
      "metadata": {
        "id": "b6db8fa2-3697-4aee-bd7a-9bdc4cab4015"
      },
      "outputs": [],
      "source": [
        "# Merge LoRA + base weights into a standalone model\n",
        "merged_model = ft_model.merge_and_unload()\n",
        "\n",
        "save_path = \"qwen2vl-2b-scienceqa-merged\"\n",
        "merged_model.save_pretrained(save_path)\n",
        "processor.save_pretrained(save_path)\n",
        "\n",
        "print(\"Saved merged model to:\", save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24e0a126-e785-4a4b-a840-3769691c4380",
      "metadata": {
        "id": "24e0a126-e785-4a4b-a840-3769691c4380",
        "outputId": "1ca287d1-46f1-490b-cdf2-04fe4ebc2641"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8986c552-cfb5-4130-a08c-d517f56d8917",
      "metadata": {
        "id": "8986c552-cfb5-4130-a08c-d517f56d8917",
        "outputId": "d07024ab-507c-4ded-e1e6-2ab697c7f550"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pydrive2\n",
            "  Downloading PyDrive2-1.21.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting google-api-python-client>=1.12.5 (from pydrive2)\n",
            "  Downloading google_api_python_client-2.187.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting oauth2client>=4.0.0 (from pydrive2)\n",
            "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.12/dist-packages (from pydrive2) (6.0.3)\n",
            "Requirement already satisfied: cryptography<44 in /usr/lib/python3/dist-packages (from pydrive2) (41.0.7)\n",
            "Collecting pyOpenSSL<=24.2.1,>=19.1.0 (from pydrive2)\n",
            "  Downloading pyOpenSSL-24.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/lib/python3/dist-packages (from google-api-python-client>=1.12.5->pydrive2) (0.20.4)\n",
            "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 (from google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading google_auth_httplib2-0.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 (from google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading google_api_core-2.28.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2) (6.33.1)\n",
            "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2) (2.32.5)\n",
            "Collecting cachetools<7.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/lib/python3/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=1.12.5->pydrive2) (3.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2) (2025.10.5)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/lib/python3/dist-packages (from oauth2client>=4.0.0->pydrive2) (1.16.0)\n",
            "Downloading PyDrive2-1.21.3-py3-none-any.whl (47 kB)\n",
            "Downloading pyOpenSSL-24.2.1-py3-none-any.whl (58 kB)\n",
            "Downloading google_api_python_client-2.187.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m138.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
            "Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
            "Downloading cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
            "Downloading google_auth_httplib2-0.2.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
            "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Installing collected packages: uritemplate, pyOpenSSL, pyasn1, proto-plus, googleapis-common-protos, cachetools, rsa, pyasn1-modules, oauth2client, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, pydrive2\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [pydrive2]/14\u001b[0m [google-api-python-client]\n",
            "\u001b[1A\u001b[2KSuccessfully installed cachetools-6.2.2 google-api-core-2.28.1 google-api-python-client-2.187.0 google-auth-2.43.0 google-auth-httplib2-0.2.1 googleapis-common-protos-1.72.0 oauth2client-4.1.3 proto-plus-1.26.1 pyOpenSSL-24.2.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydrive2-1.21.3 rsa-4.9.1 uritemplate-4.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pydrive2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "368a36a5-079c-45b4-ae4c-42e77608a925",
      "metadata": {
        "id": "368a36a5-079c-45b4-ae4c-42e77608a925",
        "outputId": "e9ddf5ea-f7ea-41b4-c09b-6940447b10b7"
      },
      "outputs": [
        {
          "ename": "InvalidConfigError",
          "evalue": "Invalid client secrets file ('Error opening file', 'client_secrets.json', 'No such file or directory', 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/oauth2client/clientsecrets.py:121\u001b[39m, in \u001b[36m_loadfile\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    122\u001b[39m         obj = json.load(fp)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'client_secrets.json'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mInvalidClientSecretsError\u001b[39m                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pydrive2/auth.py:548\u001b[39m, in \u001b[36mGoogleAuth.LoadClientConfigFile\u001b[39m\u001b[34m(self, client_config_file)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     client_type, client_info = \u001b[43mclientsecrets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloadfile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient_config_file\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m clientsecrets.InvalidClientSecretsError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/oauth2client/clientsecrets.py:165\u001b[39m, in \u001b[36mloadfile\u001b[39m\u001b[34m(filename, cache)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_loadfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m obj = cache.get(filename, namespace=_SECRET_NAMESPACE)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/oauth2client/clientsecrets.py:124\u001b[39m, in \u001b[36m_loadfile\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidClientSecretsError(\u001b[33m'\u001b[39m\u001b[33mError opening file\u001b[39m\u001b[33m'\u001b[39m, exc.filename,\n\u001b[32m    125\u001b[39m                                     exc.strerror, exc.errno)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _validate_clientsecrets(obj)\n",
            "\u001b[31mInvalidClientSecretsError\u001b[39m: ('Error opening file', 'client_secrets.json', 'No such file or directory', 2)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mInvalidConfigError\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydrive2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdrive\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GoogleDrive\n\u001b[32m      4\u001b[39m gauth = GoogleAuth()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mgauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLocalWebserverAuth\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# opens a browser to log into Google\u001b[39;00m\n\u001b[32m      6\u001b[39m drive = GoogleDrive(gauth)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pydrive2/auth.py:125\u001b[39m, in \u001b[36mCheckAuth.<locals>._decorated\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m.LoadCredentials()\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.flow \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mGetFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    127\u001b[39m     code = decoratee(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pydrive2/auth.py:644\u001b[39m, in \u001b[36mGoogleAuth.GetFlow\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Gets Flow object from client configuration.\u001b[39;00m\n\u001b[32m    638\u001b[39m \n\u001b[32m    639\u001b[39m \u001b[33;03m:raises: InvalidConfigError\u001b[39;00m\n\u001b[32m    640\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[32m    642\u001b[39m     config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client_config \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.CLIENT_CONFIGS_LIST\n\u001b[32m    643\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mLoadClientConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    645\u001b[39m constructor_kwargs = {\n\u001b[32m    646\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mredirect_uri\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.client_config[\u001b[33m\"\u001b[39m\u001b[33mredirect_uri\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    647\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mauth_uri\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.client_config[\u001b[33m\"\u001b[39m\u001b[33mauth_uri\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    648\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtoken_uri\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.client_config[\u001b[33m\"\u001b[39m\u001b[33mtoken_uri\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    649\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maccess_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33monline\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    650\u001b[39m }\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client_config[\u001b[33m\"\u001b[39m\u001b[33mrevoke_uri\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pydrive2/auth.py:528\u001b[39m, in \u001b[36mGoogleAuth.LoadClientConfig\u001b[39m\u001b[34m(self, backend)\u001b[39m\n\u001b[32m    524\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidConfigError(\n\u001b[32m    525\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease specify client config backend\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m         )\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mLoadClientConfigFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33msettings\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    530\u001b[39m     \u001b[38;5;28mself\u001b[39m.LoadClientConfigSettings()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pydrive2/auth.py:552\u001b[39m, in \u001b[36mGoogleAuth.LoadClientConfigFile\u001b[39m\u001b[34m(self, client_config_file)\u001b[39m\n\u001b[32m    548\u001b[39m     client_type, client_info = clientsecrets.loadfile(\n\u001b[32m    549\u001b[39m         client_config_file\n\u001b[32m    550\u001b[39m     )\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m clientsecrets.InvalidClientSecretsError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidConfigError(\u001b[33m\"\u001b[39m\u001b[33mInvalid client secrets file \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % error)\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m    554\u001b[39m     clientsecrets.TYPE_WEB,\n\u001b[32m    555\u001b[39m     clientsecrets.TYPE_INSTALLED,\n\u001b[32m    556\u001b[39m ):\n\u001b[32m    557\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidConfigError(\n\u001b[32m    558\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnknown client_type of client config file\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    559\u001b[39m     )\n",
            "\u001b[31mInvalidConfigError\u001b[39m: Invalid client secrets file ('Error opening file', 'client_secrets.json', 'No such file or directory', 2)"
          ]
        }
      ],
      "source": [
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "\n",
        "gauth = GoogleAuth()\n",
        "gauth.LocalWebserverAuth()  # opens a browser to log into Google\n",
        "drive = GoogleDrive(gauth)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8472009c-4c84-4772-8c10-86b708be9a9d",
      "metadata": {
        "id": "8472009c-4c84-4772-8c10-86b708be9a9d"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = \"qwen2vl-2b-scienceqa-merged\"  # change if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73acb393-1217-48f7-b605-16fec0dd082d",
      "metadata": {
        "id": "73acb393-1217-48f7-b605-16fec0dd082d",
        "outputId": "e4ad468c-bf53-483d-bd87-c5061048a94e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exists: True\n",
            "Contents: ['training_args.bin', 'video_preprocessor_config.json', 'tokenizer.json', 'merges.txt', 'vocab.json', 'added_tokens.json', 'special_tokens_map.json', 'tokenizer_config.json', 'chat_template.jinja', 'preprocessor_config.json', 'adapter_config.json', 'adapter_model.safetensors', 'checkpoint-1167', 'checkpoint-1000', 'README.md']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "path = \"/workspace/qwen2-vl-2b-scienceqa-lora-expl\"\n",
        "print(\"Exists:\", os.path.exists(path))\n",
        "print(\"Contents:\", os.listdir(path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "964d31d8-9e02-4cbb-82b3-8b1074a26e00",
      "metadata": {
        "id": "964d31d8-9e02-4cbb-82b3-8b1074a26e00",
        "outputId": "26b64b51-f3e6-49d5-c8cd-11ca1b012296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ZIP saved to: /workspace/qwen2-vl-2b-scienceqa-lora-expl.zip\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "MODEL_DIR = \"/workspace/qwen2-vl-2b-scienceqa-lora-expl\"\n",
        "ZIP_PATH = \"/workspace/qwen2-vl-2b-scienceqa-lora-expl.zip\"\n",
        "\n",
        "shutil.make_archive(\n",
        "    base_name=ZIP_PATH.replace(\".zip\", \"\"),\n",
        "    format=\"zip\",\n",
        "    root_dir=\"/workspace\",\n",
        "    base_dir=\"qwen2-vl-2b-scienceqa-lora-expl\"\n",
        ")\n",
        "\n",
        "print(\"ZIP saved to:\", ZIP_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "104201c0-ed3f-408c-87a5-07771214f2c1",
      "metadata": {
        "id": "104201c0-ed3f-408c-87a5-07771214f2c1",
        "outputId": "b7cc8c9b-1868-4f7f-85d7-49541a6c90ad"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Qwen2VLForConditionalGeneration' object has no attribute 'merge_and_unload'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m merged = \u001b[43mft_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge_and_unload\u001b[49m()\n\u001b[32m      2\u001b[39m merged.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m/workspace/qwen2-vl-2b-scienceqa-merged\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m processor.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m/workspace/qwen2-vl-2b-scienceqa-merged\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1962\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1960\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1961\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1962\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1963\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1964\u001b[39m )\n",
            "\u001b[31mAttributeError\u001b[39m: 'Qwen2VLForConditionalGeneration' object has no attribute 'merge_and_unload'"
          ]
        }
      ],
      "source": [
        "merged = ft_model.merge_and_unload()\n",
        "merged.save_pretrained(\"/workspace/qwen2-vl-2b-scienceqa-merged\")\n",
        "processor.save_pretrained(\"/workspace/qwen2-vl-2b-scienceqa-merged\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b135543e-71b2-4075-896e-9887d3319248",
      "metadata": {
        "id": "b135543e-71b2-4075-896e-9887d3319248",
        "outputId": "6a4b4f3f-04b8-41c0-c2a4-55151df9432e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zipping: /workspace/qwen2-vl-2b-scienceqa-lora-expl\n",
            "ZIP created at: /workspace/qwen2-vl-2b-scienceqa-lora-expl.zip\n"
          ]
        }
      ],
      "source": [
        "import shutil, os\n",
        "\n",
        "MODEL_DIR_NAME = \"qwen2-vl-2b-scienceqa-lora-expl\"\n",
        "ROOT_DIR = \"/workspace\"  # as shown in your screenshot\n",
        "\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, MODEL_DIR_NAME)\n",
        "ZIP_BASE = os.path.join(ROOT_DIR, MODEL_DIR_NAME)  # without .zip extension\n",
        "\n",
        "print(\"Zipping:\", MODEL_DIR)\n",
        "\n",
        "shutil.make_archive(\n",
        "    base_name=ZIP_BASE,   # /workspace/qwen2-vl-2b-scienceqa-lora-expl\n",
        "    format=\"zip\",\n",
        "    root_dir=ROOT_DIR,    # /workspace\n",
        "    base_dir=MODEL_DIR_NAME,  # qwen2-vl-2b-scienceqa-lora-expl\n",
        ")\n",
        "\n",
        "print(\"ZIP created at:\", ZIP_BASE + \".zip\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ddfefee0-06d9-4ecb-b60d-f045192b0540",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404,
          "referenced_widgets": [
            "865e319aab0748878996afbd74a64d58",
            "c4bbc6395552402fbd25f64f49340063",
            "ead9e5d8f5434b6690884d20e4997156",
            "bda564a14bbb4e3c84b7bf874908a480",
            "cb42a1dd6ead4b94b8c931dd7627106f",
            "770c6bd3d30f49a88fa99374b86afaef",
            "667cc580503d4f98a6b50ed1541744f0",
            "a8a7de06fab74a69908cab02ee081f12",
            "10f68ac707fc4e0ca551077518999d4e",
            "38e7239b30a9497b88efb2ede64c5a94",
            "979bd3c8e93c46548391c5103badcfa6",
            "29d219c01d2e4cef98d9d626d1deeccd",
            "0c8ce4b05b734896a74c74e60d64b321",
            "04b8331cb1404a9f8d8469bff43135bd",
            "76c4227c713e449cbd0a1bd4d60e248b",
            "18f2bfdeea054bedbb727efac3a32902",
            "c8b1658c070243f38acf1d607956dd86",
            "99d3784271b4465a92fbbe553b919568",
            "f23feca889194a02b2208f1c3dbc0b4c",
            "40519e8c986f4c5798e66eaea7c80e82",
            "8a9d53f3427442ab9a49bd4c601b0489",
            "6b1703cf3c0d46f89aec12716bd42662",
            "b0f48066e6184ae698174b64739b06bb",
            "bdb51a8f85514a8faf78f72dd32090cd",
            "3f4ee8d0899e405caa03a82d6b955633",
            "5b95ded97be648c2a4458965876c6da4",
            "551c5f8574034a69b669fd4b5d4d6e17",
            "7890bd7291d64ca8a6d4e0e89f2117c4",
            "1b89c84ae2634ea5ad3b9318fd8fdfef",
            "8b317ee8ec6a4d35b408a913e9adc86b",
            "14a08c48abf244a5add6652aa0354f45",
            "ad02fa42c13a4322ae4761c2a9cb4be0",
            "42a73c47ae414079a6a26087b557aab1",
            "f2d661a8e0324d3092490b6ff606e78b",
            "c4f3a51a5a6947608dba55c25a45bbe5",
            "195f9b32147748b4a60f7168ee0b6874",
            "ab98e9d4efea45209494fa5eb5b81364",
            "f43f11ef526a4ab1bcf53f49e9d730ab",
            "c9edb60e41264ac68b83d92099677387",
            "4648e0fd9027463f82ae6dd96d38633e",
            "a9dacae19ccb4ffc9bfc575e5bdc0f07",
            "29444fb871134e2e9ef623d52e2eabfa",
            "eb2cc18c006342bfb1aed53fb36af0a0",
            "2a49095360b548e6a088a9ae40b21b58",
            "e5ccbf88684e45338420058787847744",
            "6090f6f43b3d40468bfa872c260bc51d",
            "7262dc74da9c4ce886ddd3dca2bafb7c",
            "bdc0768a08364cadb09d99f95d583895",
            "09e1144425b94d12a905c8386df406d9",
            "d81a0c02e38549bb8e7d5c495b3e83be",
            "620b1892f5364b82b22f12c8c7f35c43",
            "6a3c3bfe9c8b49ddad96c74fb6c2ba30",
            "d385ca036acc45fa8ac14100e42c1e4f",
            "8740628776f84953b7200b27fe83d019",
            "75329a8204894909a64a5e269a96c3ed",
            "433ffc5ed26043338dfece3b6ce9a216",
            "f16a693fd97c46bba4e7225891cd7611",
            "8dee4868951c4fe89c9e545236084ad5",
            "f3325abf18eb423bba1bed83359bf32d",
            "636eb46816e14ec69f713ef3bbadb556",
            "03f3731568d643338c38b2acd5534457",
            "f97016928aa2492a831efc4a9391ba45",
            "b3a7448ed6d846d9961758e70137398e",
            "936b336c48fe4849a0958b5f297dc591",
            "8218ea0a086c459088f383a017634dff",
            "bcfa470418174da4b58853581f57eacf",
            "faea3ec6d7874c23a5a9b09cf79f3e7a",
            "47d2ea82ef22418c85082e5c734a83c0",
            "3c9252d96b3a488c90095ab9facc0148",
            "9b21158d1e3e4dad976a7840fa3c9795",
            "19ee2670b03445338bf402e19e4f3ad3",
            "7b3f56b8ea1b41e5bbab6e9013376ad7",
            "c01e77aa050b46809782ba0d4f51c7c6",
            "2a8817f135664be4a06aa2e292fd57df",
            "7aa48a1f6cdf4a33937ea7aa6fd71c5a",
            "b8569b7d1ef947edab67a54fe9fa8cff",
            "5af6be1f9ff9424a96dbd9c461796395"
          ]
        },
        "id": "ddfefee0-06d9-4ecb-b60d-f045192b0540",
        "outputId": "44e25919-cbcb-43c3-9f47-f80ef9e196b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "865e319aab0748878996afbd74a64d58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29d219c01d2e4cef98d9d626d1deeccd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0f48066e6184ae698174b64739b06bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2d661a8e0324d3092490b6ff606e78b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5ccbf88684e45338420058787847744",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "433ffc5ed26043338dfece3b6ce9a216",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "faea3ec6d7874c23a5a9b09cf79f3e7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer you are loading from '/content/drive/MyDrive/qwen2-vl-2b-scienceqa-lora-expl' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e.  This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        }
      ],
      "source": [
        "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "base = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# path to unzipped folder\n",
        "adapter_dir = \"/content/drive/MyDrive/qwen2-vl-2b-scienceqa-lora-expl\"\n",
        "\n",
        "model = PeftModel.from_pretrained(base, adapter_dir)\n",
        "processor = Qwen2VLProcessor.from_pretrained(adapter_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "WB-mq-vII1c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB-mq-vII1c6",
        "outputId": "fb59a8a1-095c-43bb-b24a-094d994c95b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3AlzhIzI3Bb",
      "metadata": {
        "id": "a3AlzhIzI3Bb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03f3731568d643338c38b2acd5534457": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04b8331cb1404a9f8d8469bff43135bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f23feca889194a02b2208f1c3dbc0b4c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40519e8c986f4c5798e66eaea7c80e82",
            "value": 1
          }
        },
        "09e1144425b94d12a905c8386df406d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c8ce4b05b734896a74c74e60d64b321": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8b1658c070243f38acf1d607956dd86",
            "placeholder": "​",
            "style": "IPY_MODEL_99d3784271b4465a92fbbe553b919568",
            "value": "model.safetensors.index.json: "
          }
        },
        "10f68ac707fc4e0ca551077518999d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14a08c48abf244a5add6652aa0354f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18f2bfdeea054bedbb727efac3a32902": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "195f9b32147748b4a60f7168ee0b6874": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9dacae19ccb4ffc9bfc575e5bdc0f07",
            "max": 3988609112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29444fb871134e2e9ef623d52e2eabfa",
            "value": 3988609112
          }
        },
        "19ee2670b03445338bf402e19e4f3ad3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b89c84ae2634ea5ad3b9318fd8fdfef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29444fb871134e2e9ef623d52e2eabfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29d219c01d2e4cef98d9d626d1deeccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c8ce4b05b734896a74c74e60d64b321",
              "IPY_MODEL_04b8331cb1404a9f8d8469bff43135bd",
              "IPY_MODEL_76c4227c713e449cbd0a1bd4d60e248b"
            ],
            "layout": "IPY_MODEL_18f2bfdeea054bedbb727efac3a32902"
          }
        },
        "2a49095360b548e6a088a9ae40b21b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a8817f135664be4a06aa2e292fd57df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38e7239b30a9497b88efb2ede64c5a94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c9252d96b3a488c90095ab9facc0148": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a8817f135664be4a06aa2e292fd57df",
            "max": 272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7aa48a1f6cdf4a33937ea7aa6fd71c5a",
            "value": 272
          }
        },
        "3f4ee8d0899e405caa03a82d6b955633": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b317ee8ec6a4d35b408a913e9adc86b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14a08c48abf244a5add6652aa0354f45",
            "value": 2
          }
        },
        "40519e8c986f4c5798e66eaea7c80e82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42a73c47ae414079a6a26087b557aab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "433ffc5ed26043338dfece3b6ce9a216": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f16a693fd97c46bba4e7225891cd7611",
              "IPY_MODEL_8dee4868951c4fe89c9e545236084ad5",
              "IPY_MODEL_f3325abf18eb423bba1bed83359bf32d"
            ],
            "layout": "IPY_MODEL_636eb46816e14ec69f713ef3bbadb556"
          }
        },
        "4648e0fd9027463f82ae6dd96d38633e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47d2ea82ef22418c85082e5c734a83c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b3f56b8ea1b41e5bbab6e9013376ad7",
            "placeholder": "​",
            "style": "IPY_MODEL_c01e77aa050b46809782ba0d4f51c7c6",
            "value": "generation_config.json: 100%"
          }
        },
        "551c5f8574034a69b669fd4b5d4d6e17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5af6be1f9ff9424a96dbd9c461796395": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b95ded97be648c2a4458965876c6da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad02fa42c13a4322ae4761c2a9cb4be0",
            "placeholder": "​",
            "style": "IPY_MODEL_42a73c47ae414079a6a26087b557aab1",
            "value": " 2/2 [00:12&lt;00:00, 12.34s/it]"
          }
        },
        "6090f6f43b3d40468bfa872c260bc51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d81a0c02e38549bb8e7d5c495b3e83be",
            "placeholder": "​",
            "style": "IPY_MODEL_620b1892f5364b82b22f12c8c7f35c43",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "620b1892f5364b82b22f12c8c7f35c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "636eb46816e14ec69f713ef3bbadb556": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "667cc580503d4f98a6b50ed1541744f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a3c3bfe9c8b49ddad96c74fb6c2ba30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b1703cf3c0d46f89aec12716bd42662": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7262dc74da9c4ce886ddd3dca2bafb7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a3c3bfe9c8b49ddad96c74fb6c2ba30",
            "max": 429441656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d385ca036acc45fa8ac14100e42c1e4f",
            "value": 429441656
          }
        },
        "75329a8204894909a64a5e269a96c3ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76c4227c713e449cbd0a1bd4d60e248b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a9d53f3427442ab9a49bd4c601b0489",
            "placeholder": "​",
            "style": "IPY_MODEL_6b1703cf3c0d46f89aec12716bd42662",
            "value": " 56.4k/? [00:00&lt;00:00, 6.41MB/s]"
          }
        },
        "770c6bd3d30f49a88fa99374b86afaef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7890bd7291d64ca8a6d4e0e89f2117c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aa48a1f6cdf4a33937ea7aa6fd71c5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b3f56b8ea1b41e5bbab6e9013376ad7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8218ea0a086c459088f383a017634dff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "865e319aab0748878996afbd74a64d58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4bbc6395552402fbd25f64f49340063",
              "IPY_MODEL_ead9e5d8f5434b6690884d20e4997156",
              "IPY_MODEL_bda564a14bbb4e3c84b7bf874908a480"
            ],
            "layout": "IPY_MODEL_cb42a1dd6ead4b94b8c931dd7627106f"
          }
        },
        "8740628776f84953b7200b27fe83d019": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a9d53f3427442ab9a49bd4c601b0489": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b317ee8ec6a4d35b408a913e9adc86b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dee4868951c4fe89c9e545236084ad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3a7448ed6d846d9961758e70137398e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_936b336c48fe4849a0958b5f297dc591",
            "value": 2
          }
        },
        "936b336c48fe4849a0958b5f297dc591": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "979bd3c8e93c46548391c5103badcfa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99d3784271b4465a92fbbe553b919568": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b21158d1e3e4dad976a7840fa3c9795": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8569b7d1ef947edab67a54fe9fa8cff",
            "placeholder": "​",
            "style": "IPY_MODEL_5af6be1f9ff9424a96dbd9c461796395",
            "value": " 272/272 [00:00&lt;00:00, 38.2kB/s]"
          }
        },
        "a8a7de06fab74a69908cab02ee081f12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a9dacae19ccb4ffc9bfc575e5bdc0f07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab98e9d4efea45209494fa5eb5b81364": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb2cc18c006342bfb1aed53fb36af0a0",
            "placeholder": "​",
            "style": "IPY_MODEL_2a49095360b548e6a088a9ae40b21b58",
            "value": " 3.99G/3.99G [00:11&lt;00:00, 519MB/s]"
          }
        },
        "ad02fa42c13a4322ae4761c2a9cb4be0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0f48066e6184ae698174b64739b06bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bdb51a8f85514a8faf78f72dd32090cd",
              "IPY_MODEL_3f4ee8d0899e405caa03a82d6b955633",
              "IPY_MODEL_5b95ded97be648c2a4458965876c6da4"
            ],
            "layout": "IPY_MODEL_551c5f8574034a69b669fd4b5d4d6e17"
          }
        },
        "b3a7448ed6d846d9961758e70137398e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8569b7d1ef947edab67a54fe9fa8cff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcfa470418174da4b58853581f57eacf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bda564a14bbb4e3c84b7bf874908a480": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38e7239b30a9497b88efb2ede64c5a94",
            "placeholder": "​",
            "style": "IPY_MODEL_979bd3c8e93c46548391c5103badcfa6",
            "value": " 1.20k/? [00:00&lt;00:00, 127kB/s]"
          }
        },
        "bdb51a8f85514a8faf78f72dd32090cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7890bd7291d64ca8a6d4e0e89f2117c4",
            "placeholder": "​",
            "style": "IPY_MODEL_1b89c84ae2634ea5ad3b9318fd8fdfef",
            "value": "Fetching 2 files: 100%"
          }
        },
        "bdc0768a08364cadb09d99f95d583895": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8740628776f84953b7200b27fe83d019",
            "placeholder": "​",
            "style": "IPY_MODEL_75329a8204894909a64a5e269a96c3ed",
            "value": " 429M/429M [00:01&lt;00:00, 425MB/s]"
          }
        },
        "c01e77aa050b46809782ba0d4f51c7c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4bbc6395552402fbd25f64f49340063": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_770c6bd3d30f49a88fa99374b86afaef",
            "placeholder": "​",
            "style": "IPY_MODEL_667cc580503d4f98a6b50ed1541744f0",
            "value": "config.json: "
          }
        },
        "c4f3a51a5a6947608dba55c25a45bbe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9edb60e41264ac68b83d92099677387",
            "placeholder": "​",
            "style": "IPY_MODEL_4648e0fd9027463f82ae6dd96d38633e",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "c8b1658c070243f38acf1d607956dd86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9edb60e41264ac68b83d92099677387": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb42a1dd6ead4b94b8c931dd7627106f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d385ca036acc45fa8ac14100e42c1e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d81a0c02e38549bb8e7d5c495b3e83be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5ccbf88684e45338420058787847744": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6090f6f43b3d40468bfa872c260bc51d",
              "IPY_MODEL_7262dc74da9c4ce886ddd3dca2bafb7c",
              "IPY_MODEL_bdc0768a08364cadb09d99f95d583895"
            ],
            "layout": "IPY_MODEL_09e1144425b94d12a905c8386df406d9"
          }
        },
        "ead9e5d8f5434b6690884d20e4997156": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8a7de06fab74a69908cab02ee081f12",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10f68ac707fc4e0ca551077518999d4e",
            "value": 1
          }
        },
        "eb2cc18c006342bfb1aed53fb36af0a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f16a693fd97c46bba4e7225891cd7611": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03f3731568d643338c38b2acd5534457",
            "placeholder": "​",
            "style": "IPY_MODEL_f97016928aa2492a831efc4a9391ba45",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f23feca889194a02b2208f1c3dbc0b4c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f2d661a8e0324d3092490b6ff606e78b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4f3a51a5a6947608dba55c25a45bbe5",
              "IPY_MODEL_195f9b32147748b4a60f7168ee0b6874",
              "IPY_MODEL_ab98e9d4efea45209494fa5eb5b81364"
            ],
            "layout": "IPY_MODEL_f43f11ef526a4ab1bcf53f49e9d730ab"
          }
        },
        "f3325abf18eb423bba1bed83359bf32d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8218ea0a086c459088f383a017634dff",
            "placeholder": "​",
            "style": "IPY_MODEL_bcfa470418174da4b58853581f57eacf",
            "value": " 2/2 [00:01&lt;00:00,  1.62it/s]"
          }
        },
        "f43f11ef526a4ab1bcf53f49e9d730ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f97016928aa2492a831efc4a9391ba45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "faea3ec6d7874c23a5a9b09cf79f3e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47d2ea82ef22418c85082e5c734a83c0",
              "IPY_MODEL_3c9252d96b3a488c90095ab9facc0148",
              "IPY_MODEL_9b21158d1e3e4dad976a7840fa3c9795"
            ],
            "layout": "IPY_MODEL_19ee2670b03445338bf402e19e4f3ad3"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
